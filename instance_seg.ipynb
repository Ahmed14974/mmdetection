{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Trial and Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: nvcc: command not found\n",
      "gcc (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609\n",
      "Copyright (C) 2015 Free Software Foundation, Inc.\n",
      "This is free software; see the source for copying conditions.  There is NO\n",
      "warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check nvcc version\n",
    "!nvcc -V\n",
    "# Check GCC version\n",
    "!gcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11.0 False\n",
      "2.25.0\n",
      "11.3\n",
      "GCC 9.3\n"
     ]
    }
   ],
   "source": [
    "# Check Pytorch installation\n",
    "import torch, torchvision\n",
    "print(torch.__version__, torch.cuda.is_available())\n",
    "\n",
    "# Check MMDetection installation\n",
    "import mmdet\n",
    "print(mmdet.__version__)\n",
    "\n",
    "# Check mmcv installation\n",
    "from mmcv.ops import get_compiling_cuda_version, get_compiler_version\n",
    "print(get_compiling_cuda_version())\n",
    "print(get_compiler_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mmcv\n",
    "from mmcv.runner import load_checkpoint\n",
    "\n",
    "from mmdet.apis import inference_detector, show_result_pyplot\n",
    "from mmdet.models import build_detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import os.path as osp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation = mmcv.load(image_prefix + jsons[0])\n",
    "\n",
    "annotations = []\n",
    "images = []\n",
    "obj_count = 0\n",
    "\n",
    "idx = 0\n",
    "\n",
    "filename = str(annotation['imagePath'])\n",
    "# print(filename)\n",
    "img_path = osp.join(image_prefix, filename)\n",
    "# print(img_path)\n",
    "height, width = int(annotation['imageHeight']), int(annotation['imageWidth'])\n",
    "# print(height, width)\n",
    "\n",
    "images.append(dict(\n",
    "    id=int(idx),\n",
    "    file_name=filename,\n",
    "    height=height,\n",
    "    width=width))\n",
    "\n",
    "bboxes = []\n",
    "labels = []\n",
    "masks = []\n",
    "for obj in annotation['shapes']:\n",
    "    px = [points[0] for points in obj['points']]\n",
    "    py = [points[1] for points in obj['points']]\n",
    "    poly = [(x + 0.5, y + 0.5) for x, y in zip(px, py)]\n",
    "    poly = [p for x in poly for p in x]\n",
    "\n",
    "    x_min, y_min, x_max, y_max = (\n",
    "        min(px), min(py), max(px), max(py))\n",
    "\n",
    "    data_anno = dict(\n",
    "        image_id=idx,\n",
    "        id=obj_count,\n",
    "        category_id=0,\n",
    "        bbox=[x_min, y_min, x_max - x_min, y_max - y_min],\n",
    "        area=(x_max - x_min) * (y_max - y_min),\n",
    "        segmentation=[poly],\n",
    "        iscrowd=0)\n",
    "    annotations.append(data_anno)\n",
    "    obj_count += 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "bboxes = []\n",
    "labels = []\n",
    "masks = []\n",
    "for obj in annotation['shapes']:\n",
    "    px = [points[0] for points in obj['points']]\n",
    "    py = [points[1] for points in obj['points']]\n",
    "    poly = [(x + 0.5, y + 0.5) for x, y in zip(px, py)]\n",
    "    poly = [p for x in poly for p in x]\n",
    "\n",
    "    x_min, y_min, x_max, y_max = (\n",
    "        min(px), min(py), max(px), max(py))\n",
    "\n",
    "    data_anno = dict(\n",
    "        image_id=idx,\n",
    "        id=obj_count,\n",
    "        category_id=0,\n",
    "        bbox=[x_min, y_min, x_max - x_min, y_max - y_min],\n",
    "        area=(x_max - x_min) * (y_max - y_min),\n",
    "        segmentation=[poly],\n",
    "        iscrowd=0)\n",
    "    annotations.append(data_anno)\n",
    "    obj_count += 1    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories=[{'id':0, 'name': 'shiptrack'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmcv import Config\n",
    "cfg = Config.fromfile('/deep/u/mahmedc/mmdetection/configs/solov2/solov2_light_r18_fpn_3x_coco.py')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating Separate Directories for Train, Val, Test Image Copies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import shutil\n",
    "from shutil import copyfile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '/deep/group/aicc-bootcamp/cloud-pollution/data/combined_v3_typed_new_composite/COCO_format/'\n",
    "train_path = DATA_DIR + 'train.csv'\n",
    "val_path = DATA_DIR + 'val.csv'\n",
    "test_path = DATA_DIR + 'test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv = pd.read_csv(train_path)\n",
    "\n",
    "train_img_dir = DATA_DIR + 'train/images/'\n",
    "\n",
    "for item in train_csv.loc[:,'images_only']:\n",
    "    image_name = item.split(\"/\")[-1]\n",
    "    shutil.copyfile(item, train_img_dir + image_name)\n",
    "\n",
    "train_jsons_dir = DATA_DIR + 'train/jsons/'\n",
    "\n",
    "for item in train_csv.loc[:,'jsons_only']:\n",
    "    json_name = item.split(\"/\")[-1]\n",
    "    shutil.copyfile(item, train_jsons_dir + json_name)\n",
    "\n",
    "train_masks_dir = DATA_DIR + 'train/masks/'\n",
    "\n",
    "for item in train_csv.loc[:,'masks_updated']:\n",
    "    mask_name = item.split(\"/\")[-1]\n",
    "    shutil.copyfile(item, train_masks_dir + mask_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_csv = pd.read_csv(val_path)\n",
    "\n",
    "val_img_dir = DATA_DIR + 'val/images/'\n",
    "\n",
    "for item in val_csv.loc[:,'images_only']:\n",
    "    image_name = item.split(\"/\")[-1]\n",
    "    shutil.copyfile(item, val_img_dir + image_name)\n",
    "\n",
    "val_jsons_dir = DATA_DIR + 'val/jsons/'\n",
    "\n",
    "for item in val_csv.loc[:,'jsons_only']:\n",
    "    json_name = item.split(\"/\")[-1]\n",
    "    shutil.copyfile(item, val_jsons_dir + json_name)\n",
    "\n",
    "val_masks_dir = DATA_DIR + 'val/masks/'\n",
    "\n",
    "for item in val_csv.loc[:,'masks_updated']:\n",
    "    mask_name = item.split(\"/\")[-1]\n",
    "    shutil.copyfile(item, val_masks_dir + mask_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_csv = pd.read_csv(test_path)\n",
    "\n",
    "test_img_dir = DATA_DIR + 'test/images/'\n",
    "\n",
    "for item in test_csv.loc[:,'images_only']:\n",
    "    image_name = item.split(\"/\")[-1]\n",
    "    shutil.copyfile(item, test_img_dir + image_name)\n",
    "\n",
    "test_jsons_dir = DATA_DIR + 'test/jsons/'\n",
    "\n",
    "for item in test_csv.loc[:,'jsons_only']:\n",
    "    json_name = item.split(\"/\")[-1]\n",
    "    shutil.copyfile(item, test_jsons_dir + json_name)\n",
    "\n",
    "test_masks_dir = DATA_DIR + 'test/masks/'\n",
    "\n",
    "for item in test_csv.loc[:,'masks_updated']:\n",
    "    mask_name = item.split(\"/\")[-1]\n",
    "    shutil.copyfile(item, test_masks_dir + mask_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Converting Train, Val, Test to COCO Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting all train json files\n",
    "train_jsons = [f for f in os.listdir(train_jsons_dir)]\n",
    "\n",
    "# Train Annotations\n",
    "\n",
    "annotations = []\n",
    "images = []\n",
    "obj_count = 0\n",
    "for idx in range(len(train_jsons)):\n",
    "    annotation = mmcv.load(train_jsons_dir + train_jsons[idx])\n",
    "\n",
    "    filename = annotation['imagePath']\n",
    "    # print(filename)\n",
    "    # img_path = osp.join(image_prefix, filename)\n",
    "    # print(img_path)\n",
    "    height, width = int(annotation['imageHeight']), int(annotation['imageWidth'])\n",
    "    # print(height, width)\n",
    "\n",
    "    images.append(dict(\n",
    "    id=int(idx),\n",
    "    file_name=filename,\n",
    "    height=height,\n",
    "    width=width))\n",
    "\n",
    "    bboxes = []\n",
    "    labels = []\n",
    "    masks = []\n",
    "    for obj in annotation['shapes']:\n",
    "        px = [points[0] for points in obj['points']]\n",
    "        py = [points[1] for points in obj['points']]\n",
    "        poly = [(x + 0.5, y + 0.5) for x, y in zip(px, py)]\n",
    "        poly = [p for x in poly for p in x]\n",
    "\n",
    "        x_min, y_min, x_max, y_max = (\n",
    "            min(px), min(py), max(px), max(py))\n",
    "\n",
    "        data_anno = dict(\n",
    "            image_id=idx,\n",
    "            id=obj_count,\n",
    "            category_id=0,\n",
    "            bbox=[x_min, y_min, x_max - x_min, y_max - y_min],\n",
    "            area=(x_max - x_min) * (y_max - y_min),\n",
    "            segmentation=[poly],\n",
    "            iscrowd=0)\n",
    "        annotations.append(data_anno)\n",
    "        obj_count += 1    \n",
    "\n",
    "out_file = DATA_DIR + 'train/train_annotation_coco.json'\n",
    "coco_format_json = dict(\n",
    "    images=images,\n",
    "    annotations=annotations,\n",
    "    categories=[{'id':0, 'name': 'shiptrack'}])\n",
    "\n",
    "mmcv.dump(coco_format_json, out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting all val json files\n",
    "val_jsons = [f for f in os.listdir(val_jsons_dir)]\n",
    "\n",
    "# Val Annotations\n",
    "\n",
    "annotations = []\n",
    "images = []\n",
    "obj_count = 0\n",
    "for idx in range(len(val_jsons)):\n",
    "    annotation = mmcv.load(val_jsons_dir + val_jsons[idx])\n",
    "\n",
    "    filename = annotation['imagePath']\n",
    "    # print(filename)\n",
    "    # img_path = osp.join(image_prefix, filename)\n",
    "    # print(img_path)\n",
    "    height, width = int(annotation['imageHeight']), int(annotation['imageWidth'])\n",
    "    # print(height, width)\n",
    "\n",
    "    images.append(dict(\n",
    "    id=int(idx),\n",
    "    file_name=filename,\n",
    "    height=height,\n",
    "    width=width))\n",
    "\n",
    "    bboxes = []\n",
    "    labels = []\n",
    "    masks = []\n",
    "    for obj in annotation['shapes']:\n",
    "        px = [points[0] for points in obj['points']]\n",
    "        py = [points[1] for points in obj['points']]\n",
    "        poly = [(x + 0.5, y + 0.5) for x, y in zip(px, py)]\n",
    "        poly = [p for x in poly for p in x]\n",
    "\n",
    "        x_min, y_min, x_max, y_max = (\n",
    "            min(px), min(py), max(px), max(py))\n",
    "\n",
    "        data_anno = dict(\n",
    "            image_id=idx,\n",
    "            id=obj_count,\n",
    "            category_id=0,\n",
    "            bbox=[x_min, y_min, x_max - x_min, y_max - y_min],\n",
    "            area=(x_max - x_min) * (y_max - y_min),\n",
    "            segmentation=[poly],\n",
    "            iscrowd=0)\n",
    "        annotations.append(data_anno)\n",
    "        obj_count += 1    \n",
    "\n",
    "out_file = DATA_DIR + 'val/val_annotation_coco.json'\n",
    "coco_format_json = dict(\n",
    "    images=images,\n",
    "    annotations=annotations,\n",
    "    categories=[{'id':0, 'name': 'shiptrack'}])\n",
    "\n",
    "mmcv.dump(coco_format_json, out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting all test json files\n",
    "test_jsons = [f for f in os.listdir(test_jsons_dir)]\n",
    "\n",
    "# Test Annotations\n",
    "\n",
    "annotations = []\n",
    "images = []\n",
    "obj_count = 0\n",
    "for idx in range(len(test_jsons)):\n",
    "    annotation = mmcv.load(test_jsons_dir + test_jsons[idx])\n",
    "\n",
    "    filename = annotation['imagePath']\n",
    "    # print(filename)\n",
    "    # img_path = osp.join(image_prefix, filename)\n",
    "    # print(img_path)\n",
    "    height, width = int(annotation['imageHeight']), int(annotation['imageWidth'])\n",
    "    # print(height, width)\n",
    "\n",
    "    images.append(dict(\n",
    "    id=int(idx),\n",
    "    file_name=filename,\n",
    "    height=height,\n",
    "    width=width))\n",
    "\n",
    "    bboxes = []\n",
    "    labels = []\n",
    "    masks = []\n",
    "    for obj in annotation['shapes']:\n",
    "        px = [points[0] for points in obj['points']]\n",
    "        py = [points[1] for points in obj['points']]\n",
    "        poly = [(x + 0.5, y + 0.5) for x, y in zip(px, py)]\n",
    "        poly = [p for x in poly for p in x]\n",
    "\n",
    "        x_min, y_min, x_max, y_max = (\n",
    "            min(px), min(py), max(px), max(py))\n",
    "\n",
    "        data_anno = dict(\n",
    "            image_id=idx,\n",
    "            id=obj_count,\n",
    "            category_id=0,\n",
    "            bbox=[x_min, y_min, x_max - x_min, y_max - y_min],\n",
    "            area=(x_max - x_min) * (y_max - y_min),\n",
    "            segmentation=[poly],\n",
    "            iscrowd=0)\n",
    "        annotations.append(data_anno)\n",
    "        obj_count += 1    \n",
    "\n",
    "out_file = DATA_DIR + 'test/test_annotation_coco.json'\n",
    "coco_format_json = dict(\n",
    "    images=images,\n",
    "    annotations=annotations,\n",
    "    categories=[{'id':0, 'name': 'shiptrack'}])\n",
    "\n",
    "mmcv.dump(coco_format_json, out_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Repeating Data Processing Steps above for Cropped JSON Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nothing done.\n"
     ]
    }
   ],
   "source": [
    "%reset\n",
    "import pandas as pd\n",
    "import string\n",
    "import shutil\n",
    "from shutil import copyfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '/deep/group/aicc-bootcamp/cloud-pollution/data/combined_v3_typed_new_composite/COCO_format_cropped/'\n",
    "train_path = DATA_DIR + 'train.csv'\n",
    "val_path = DATA_DIR + 'val.csv'\n",
    "test_path = DATA_DIR + 'test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv = pd.read_csv(train_path)\n",
    "\n",
    "train_img_dir = DATA_DIR + 'train/images/'\n",
    "\n",
    "for item in train_csv.loc[:,'image']:\n",
    "    image_name = item.split(\"/\")[-1]\n",
    "    shutil.copyfile(item, train_img_dir + image_name)\n",
    "\n",
    "train_jsons_dir = DATA_DIR + 'train/jsons/'\n",
    "\n",
    "for item in train_csv.loc[:,'json']:\n",
    "    json_name = item.split(\"/\")[-1]\n",
    "    shutil.copyfile(item, train_jsons_dir + json_name)\n",
    "\n",
    "train_masks_dir = DATA_DIR + 'train/masks/'\n",
    "\n",
    "for item in train_csv.loc[:,'mask']:\n",
    "    mask_name = item.split(\"/\")[-1]\n",
    "    shutil.copyfile(item, train_masks_dir + mask_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_csv = pd.read_csv(val_path)\n",
    "\n",
    "val_img_dir = DATA_DIR + 'val/images/'\n",
    "\n",
    "for item in val_csv.loc[:,'image']:\n",
    "    image_name = item.split(\"/\")[-1]\n",
    "    shutil.copyfile(item, val_img_dir + image_name)\n",
    "\n",
    "val_jsons_dir = DATA_DIR + 'val/jsons/'\n",
    "\n",
    "for item in val_csv.loc[:,'json']:\n",
    "    json_name = item.split(\"/\")[-1]\n",
    "    shutil.copyfile(item, val_jsons_dir + json_name)\n",
    "\n",
    "val_masks_dir = DATA_DIR + 'val/masks/'\n",
    "\n",
    "for item in val_csv.loc[:,'mask']:\n",
    "    mask_name = item.split(\"/\")[-1]\n",
    "    shutil.copyfile(item, val_masks_dir + mask_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_csv = pd.read_csv(test_path)\n",
    "\n",
    "test_img_dir = DATA_DIR + 'test/images/'\n",
    "\n",
    "for item in test_csv.loc[:,'image']:\n",
    "    image_name = item.split(\"/\")[-1]\n",
    "    shutil.copyfile(item, test_img_dir + image_name)\n",
    "\n",
    "test_jsons_dir = DATA_DIR + 'test/jsons/'\n",
    "\n",
    "for item in test_csv.loc[:,'json']:\n",
    "    json_name = item.split(\"/\")[-1]\n",
    "    shutil.copyfile(item, test_jsons_dir + json_name)\n",
    "\n",
    "test_masks_dir = DATA_DIR + 'test/masks/'\n",
    "\n",
    "for item in test_csv.loc[:,'mask']:\n",
    "    mask_name = item.split(\"/\")[-1]\n",
    "    shutil.copyfile(item, test_masks_dir + mask_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_jsons_dir = DATA_DIR + 'train/jsons/'\n",
    "\n",
    "# Getting all train json files\n",
    "train_jsons = [f for f in os.listdir(train_jsons_dir)]\n",
    "\n",
    "# Train Annotations\n",
    "\n",
    "annotations = []\n",
    "images = []\n",
    "obj_count = 0\n",
    "for idx in range(len(train_jsons)):\n",
    "    try:\n",
    "        annotation = mmcv.load(train_jsons_dir + train_jsons[idx])\n",
    "\n",
    "        filename = annotation['imagePath']\n",
    "        # print(filename)\n",
    "        # img_path = osp.join(image_prefix, filename)\n",
    "        # print(img_path)\n",
    "        height, width = int(annotation['imageHeight']), int(annotation['imageWidth'])\n",
    "        # print(height, width)\n",
    "\n",
    "        images.append(dict(\n",
    "        id=int(idx),\n",
    "        file_name=filename,\n",
    "        height=height,\n",
    "        width=width))\n",
    "\n",
    "        bboxes = []\n",
    "        labels = []\n",
    "        masks = []\n",
    "        if len(annotation['shapes']) > 0:\n",
    "            for obj in annotation['shapes']:\n",
    "                px = [points[0] for points in obj['points']]\n",
    "                py = [points[1] for points in obj['points']]\n",
    "                poly = [(x + 0.5, y + 0.5) for x, y in zip(px, py)]\n",
    "                poly = [p for x in poly for p in x]\n",
    "\n",
    "                x_min, y_min, x_max, y_max = (\n",
    "                    min(px), min(py), max(px), max(py))\n",
    "\n",
    "                data_anno = dict(\n",
    "                    image_id=idx,\n",
    "                    id=obj_count,\n",
    "                    category_id=0,\n",
    "                    bbox=[x_min, y_min, x_max - x_min, y_max - y_min],\n",
    "                    area=(x_max - x_min) * (y_max - y_min),\n",
    "                    segmentation=[poly],\n",
    "                    iscrowd=0)\n",
    "                annotations.append(data_anno)\n",
    "                obj_count += 1    \n",
    "        # else:    \n",
    "            # No annotations associated in this case\n",
    "            \n",
    "            # data_anno = dict(\n",
    "            #     image_id=idx,\n",
    "            #     id='',\n",
    "            #     category_id=0,\n",
    "            #     bbox=[],\n",
    "            #     area=0,\n",
    "            #     segmentation=[],\n",
    "            #     iscrowd=0)\n",
    "            # annotations.append(data_anno)\n",
    "            # obj_count += 1    \n",
    "\n",
    "    except: \n",
    "        print(train_jsons[idx])\n",
    "    \n",
    "out_file = DATA_DIR + 'train/train_annotation_coco.json'\n",
    "coco_format_json = dict(\n",
    "    images=images,\n",
    "    annotations=annotations,\n",
    "    categories=[{'id':0, 'name': 'shiptrack'}])\n",
    "\n",
    "mmcv.dump(coco_format_json, out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_jsons_dir = DATA_DIR + 'val/jsons/'\n",
    "\n",
    "# Getting all val json files\n",
    "val_jsons = [f for f in os.listdir(val_jsons_dir)]\n",
    "\n",
    "# Val Annotations\n",
    "\n",
    "annotations = []\n",
    "images = []\n",
    "obj_count = 0\n",
    "for idx in range(len(val_jsons)):\n",
    "    try:\n",
    "        annotation = mmcv.load(val_jsons_dir + val_jsons[idx])\n",
    "\n",
    "        filename = annotation['imagePath']\n",
    "        # print(filename)\n",
    "        # img_path = osp.join(image_prefix, filename)\n",
    "        # print(img_path)\n",
    "        height, width = int(annotation['imageHeight']), int(annotation['imageWidth'])\n",
    "        # print(height, width)\n",
    "\n",
    "        images.append(dict(\n",
    "        id=int(idx),\n",
    "        file_name=filename,\n",
    "        height=height,\n",
    "        width=width))\n",
    "\n",
    "        bboxes = []\n",
    "        labels = []\n",
    "        masks = []\n",
    "        if len(annotation['shapes']) > 0:\n",
    "            for obj in annotation['shapes']:\n",
    "                px = [points[0] for points in obj['points']]\n",
    "                py = [points[1] for points in obj['points']]\n",
    "                poly = [(x + 0.5, y + 0.5) for x, y in zip(px, py)]\n",
    "                poly = [p for x in poly for p in x]\n",
    "\n",
    "                x_min, y_min, x_max, y_max = (\n",
    "                    min(px), min(py), max(px), max(py))\n",
    "\n",
    "                data_anno = dict(\n",
    "                    image_id=idx,\n",
    "                    id=obj_count,\n",
    "                    category_id=0,\n",
    "                    bbox=[x_min, y_min, x_max - x_min, y_max - y_min],\n",
    "                    area=(x_max - x_min) * (y_max - y_min),\n",
    "                    segmentation=[poly],\n",
    "                    iscrowd=0)\n",
    "                annotations.append(data_anno)\n",
    "                obj_count += 1    \n",
    "        # else:    \n",
    "            # No annotations associated in this case\n",
    "            \n",
    "            # data_anno = dict(\n",
    "            #     image_id=idx,\n",
    "            #     id='',\n",
    "            #     category_id=0,\n",
    "            #     bbox=[],\n",
    "            #     area=0,\n",
    "            #     segmentation=[],\n",
    "            #     iscrowd=0)\n",
    "            # annotations.append(data_anno)\n",
    "            # obj_count += 1    \n",
    "\n",
    "    except: \n",
    "        print(val_jsons[idx])\n",
    "    \n",
    "out_file = DATA_DIR + 'val/val_annotation_coco.json'\n",
    "coco_format_json = dict(\n",
    "    images=images,\n",
    "    annotations=annotations,\n",
    "    categories=[{'id':0, 'name': 'shiptrack'}])\n",
    "\n",
    "mmcv.dump(coco_format_json, out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_jsons_dir = DATA_DIR + 'test/jsons/'\n",
    "\n",
    "# Getting all test json files\n",
    "test_jsons = [f for f in os.listdir(test_jsons_dir)]\n",
    "\n",
    "# Test Annotations\n",
    "\n",
    "annotations = []\n",
    "images = []\n",
    "obj_count = 0\n",
    "for idx in range(len(test_jsons)):\n",
    "    try:\n",
    "        annotation = mmcv.load(test_jsons_dir + test_jsons[idx])\n",
    "\n",
    "        filename = annotation['imagePath']\n",
    "        # print(filename)\n",
    "        # img_path = osp.join(image_prefix, filename)\n",
    "        # print(img_path)\n",
    "        height, width = int(annotation['imageHeight']), int(annotation['imageWidth'])\n",
    "        # print(height, width)\n",
    "\n",
    "        images.append(dict(\n",
    "        id=int(idx),\n",
    "        file_name=filename,\n",
    "        height=height,\n",
    "        width=width))\n",
    "\n",
    "        bboxes = []\n",
    "        labels = []\n",
    "        masks = []\n",
    "        if len(annotation['shapes']) > 0:\n",
    "            for obj in annotation['shapes']:\n",
    "                px = [points[0] for points in obj['points']]\n",
    "                py = [points[1] for points in obj['points']]\n",
    "                poly = [(x + 0.5, y + 0.5) for x, y in zip(px, py)]\n",
    "                poly = [p for x in poly for p in x]\n",
    "\n",
    "                x_min, y_min, x_max, y_max = (\n",
    "                    min(px), min(py), max(px), max(py))\n",
    "\n",
    "                data_anno = dict(\n",
    "                    image_id=idx,\n",
    "                    id=obj_count,\n",
    "                    category_id=0,\n",
    "                    bbox=[x_min, y_min, x_max - x_min, y_max - y_min],\n",
    "                    area=(x_max - x_min) * (y_max - y_min),\n",
    "                    segmentation=[poly],\n",
    "                    iscrowd=0)\n",
    "                annotations.append(data_anno)\n",
    "                obj_count += 1    \n",
    "        # else:    \n",
    "            # No annotations associated in this case\n",
    "            \n",
    "            # data_anno = dict(\n",
    "            #     image_id=idx,\n",
    "            #     id='',\n",
    "            #     category_id=0,\n",
    "            #     bbox=[],\n",
    "            #     area=0,\n",
    "            #     segmentation=[],\n",
    "            #     iscrowd=0)\n",
    "            # annotations.append(data_anno)\n",
    "            # obj_count += 1    \n",
    "\n",
    "    except: \n",
    "        print(test_jsons[idx])\n",
    "    \n",
    "out_file = DATA_DIR + 'test/test_annotation_coco.json'\n",
    "coco_format_json = dict(\n",
    "    images=images,\n",
    "    annotations=annotations,\n",
    "    categories=[{'id':0, 'name': 'shiptrack'}])\n",
    "\n",
    "mmcv.dump(coco_format_json, out_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Repeating above steps keeping only positives for Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nothing done.\n"
     ]
    }
   ],
   "source": [
    "%reset\n",
    "import pandas as pd\n",
    "import string\n",
    "import shutil\n",
    "from shutil import copyfile\n",
    "import mmcv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '/deep/group/aicc-bootcamp/cloud-pollution/data/combined_v3_typed_new_composite/COCO_format_cropped_postrain/'\n",
    "train_path = DATA_DIR + 'train.csv'\n",
    "val_path = DATA_DIR + 'val.csv'\n",
    "test_path = DATA_DIR + 'test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv = pd.read_csv(train_path)\n",
    "\n",
    "train_img_dir = DATA_DIR + 'train/images/'\n",
    "train_jsons_dir = DATA_DIR + 'train/jsons/'\n",
    "train_masks_dir = DATA_DIR + 'train/masks/'\n",
    "\n",
    "for idx in range(len(train_csv)):\n",
    "    if train_csv.loc[idx, 'contains_shiptrack'] == True:\n",
    "        image = train_csv.loc[idx,'image']\n",
    "        image_name = image.split(\"/\")[-1]\n",
    "        shutil.copyfile(image, train_img_dir + image_name)\n",
    "        \n",
    "        json = train_csv.loc[idx,'json']\n",
    "        json_name = json.split(\"/\")[-1]\n",
    "        shutil.copyfile(json, train_jsons_dir + json_name)\n",
    "\n",
    "        mask = train_csv.loc[idx,'mask']\n",
    "        mask_name = mask.split(\"/\")[-1]\n",
    "        shutil.copyfile(mask, train_masks_dir + mask_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_csv = pd.read_csv(val_path)\n",
    "\n",
    "val_img_dir = DATA_DIR + 'val/images/'\n",
    "\n",
    "for item in val_csv.loc[:,'image']:\n",
    "    image_name = item.split(\"/\")[-1]\n",
    "    shutil.copyfile(item, val_img_dir + image_name)\n",
    "\n",
    "val_jsons_dir = DATA_DIR + 'val/jsons/'\n",
    "\n",
    "for item in val_csv.loc[:,'json']:\n",
    "    json_name = item.split(\"/\")[-1]\n",
    "    shutil.copyfile(item, val_jsons_dir + json_name)\n",
    "\n",
    "val_masks_dir = DATA_DIR + 'val/masks/'\n",
    "\n",
    "for item in val_csv.loc[:,'mask']:\n",
    "    mask_name = item.split(\"/\")[-1]\n",
    "    shutil.copyfile(item, val_masks_dir + mask_name)\n",
    "\n",
    "\n",
    "test_csv = pd.read_csv(test_path)\n",
    "\n",
    "test_img_dir = DATA_DIR + 'test/images/'\n",
    "\n",
    "for item in test_csv.loc[:,'image']:\n",
    "    image_name = item.split(\"/\")[-1]\n",
    "    shutil.copyfile(item, test_img_dir + image_name)\n",
    "\n",
    "test_jsons_dir = DATA_DIR + 'test/jsons/'\n",
    "\n",
    "for item in test_csv.loc[:,'json']:\n",
    "    json_name = item.split(\"/\")[-1]\n",
    "    shutil.copyfile(item, test_jsons_dir + json_name)\n",
    "\n",
    "test_masks_dir = DATA_DIR + 'test/masks/'\n",
    "\n",
    "for item in test_csv.loc[:,'mask']:\n",
    "    mask_name = item.split(\"/\")[-1]\n",
    "    shutil.copyfile(item, test_masks_dir + mask_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_jsons_dir = DATA_DIR + 'train/jsons/'\n",
    "\n",
    "# Getting all train json files\n",
    "train_jsons = [f for f in os.listdir(train_jsons_dir)]\n",
    "\n",
    "# Train Annotations\n",
    "\n",
    "annotations = []\n",
    "images = []\n",
    "obj_count = 0\n",
    "for idx in range(len(train_jsons)):\n",
    "    try:\n",
    "        annotation = mmcv.load(train_jsons_dir + train_jsons[idx])\n",
    "\n",
    "        filename = annotation['imagePath']\n",
    "        # print(filename)\n",
    "        # img_path = osp.join(image_prefix, filename)\n",
    "        # print(img_path)\n",
    "        height, width = int(annotation['imageHeight']), int(annotation['imageWidth'])\n",
    "        # print(height, width)\n",
    "\n",
    "        images.append(dict(\n",
    "        id=int(idx),\n",
    "        file_name=filename,\n",
    "        height=height,\n",
    "        width=width))\n",
    "\n",
    "        bboxes = []\n",
    "        labels = []\n",
    "        masks = []\n",
    "        if len(annotation['shapes']) > 0:\n",
    "            for obj in annotation['shapes']:\n",
    "                px = [points[0] for points in obj['points']]\n",
    "                py = [points[1] for points in obj['points']]\n",
    "                poly = [(x + 0.5, y + 0.5) for x, y in zip(px, py)]\n",
    "                poly = [p for x in poly for p in x]\n",
    "\n",
    "                x_min, y_min, x_max, y_max = (\n",
    "                    min(px), min(py), max(px), max(py))\n",
    "\n",
    "                data_anno = dict(\n",
    "                    image_id=idx,\n",
    "                    id=obj_count,\n",
    "                    category_id=0,\n",
    "                    bbox=[x_min, y_min, x_max - x_min, y_max - y_min],\n",
    "                    area=(x_max - x_min) * (y_max - y_min),\n",
    "                    segmentation=[poly],\n",
    "                    iscrowd=0)\n",
    "                annotations.append(data_anno)\n",
    "                obj_count += 1    \n",
    "        # else:    \n",
    "            # No annotations associated in this case\n",
    "            \n",
    "            # data_anno = dict(\n",
    "            #     image_id=idx,\n",
    "            #     id='',\n",
    "            #     category_id=0,\n",
    "            #     bbox=[],\n",
    "            #     area=0,\n",
    "            #     segmentation=[],\n",
    "            #     iscrowd=0)\n",
    "            # annotations.append(data_anno)\n",
    "            # obj_count += 1    \n",
    "\n",
    "    except: \n",
    "        print(train_jsons[idx])\n",
    "    \n",
    "out_file = DATA_DIR + 'train/train_annotation_coco.json'\n",
    "coco_format_json = dict(\n",
    "    images=images,\n",
    "    annotations=annotations,\n",
    "    categories=[{'id':0, 'name': 'shiptrack'}])\n",
    "\n",
    "mmcv.dump(coco_format_json, out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_jsons_dir = DATA_DIR + 'val/jsons/'\n",
    "\n",
    "# Getting all val json files\n",
    "val_jsons = [f for f in os.listdir(val_jsons_dir)]\n",
    "\n",
    "# Val Annotations\n",
    "\n",
    "annotations = []\n",
    "images = []\n",
    "obj_count = 0\n",
    "for idx in range(len(val_jsons)):\n",
    "    try:\n",
    "        annotation = mmcv.load(val_jsons_dir + val_jsons[idx])\n",
    "\n",
    "        filename = annotation['imagePath']\n",
    "        # print(filename)\n",
    "        # img_path = osp.join(image_prefix, filename)\n",
    "        # print(img_path)\n",
    "        height, width = int(annotation['imageHeight']), int(annotation['imageWidth'])\n",
    "        # print(height, width)\n",
    "\n",
    "        images.append(dict(\n",
    "        id=int(idx),\n",
    "        file_name=filename,\n",
    "        height=height,\n",
    "        width=width))\n",
    "\n",
    "        bboxes = []\n",
    "        labels = []\n",
    "        masks = []\n",
    "        if len(annotation['shapes']) > 0:\n",
    "            for obj in annotation['shapes']:\n",
    "                px = [points[0] for points in obj['points']]\n",
    "                py = [points[1] for points in obj['points']]\n",
    "                poly = [(x + 0.5, y + 0.5) for x, y in zip(px, py)]\n",
    "                poly = [p for x in poly for p in x]\n",
    "\n",
    "                x_min, y_min, x_max, y_max = (\n",
    "                    min(px), min(py), max(px), max(py))\n",
    "\n",
    "                data_anno = dict(\n",
    "                    image_id=idx,\n",
    "                    id=obj_count,\n",
    "                    category_id=0,\n",
    "                    bbox=[x_min, y_min, x_max - x_min, y_max - y_min],\n",
    "                    area=(x_max - x_min) * (y_max - y_min),\n",
    "                    segmentation=[poly],\n",
    "                    iscrowd=0)\n",
    "                annotations.append(data_anno)\n",
    "                obj_count += 1    \n",
    "        # else:    \n",
    "            # No annotations associated in this case\n",
    "            \n",
    "            # data_anno = dict(\n",
    "            #     image_id=idx,\n",
    "            #     id='',\n",
    "            #     category_id=0,\n",
    "            #     bbox=[],\n",
    "            #     area=0,\n",
    "            #     segmentation=[],\n",
    "            #     iscrowd=0)\n",
    "            # annotations.append(data_anno)\n",
    "            # obj_count += 1    \n",
    "\n",
    "    except: \n",
    "        print(val_jsons[idx])\n",
    "    \n",
    "out_file = DATA_DIR + 'val/val_annotation_coco.json'\n",
    "coco_format_json = dict(\n",
    "    images=images,\n",
    "    annotations=annotations,\n",
    "    categories=[{'id':0, 'name': 'shiptrack'}])\n",
    "\n",
    "mmcv.dump(coco_format_json, out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_jsons_dir = DATA_DIR + 'test/jsons/'\n",
    "\n",
    "# Getting all test json files\n",
    "test_jsons = [f for f in os.listdir(test_jsons_dir)]\n",
    "\n",
    "# Test Annotations\n",
    "\n",
    "annotations = []\n",
    "images = []\n",
    "obj_count = 0\n",
    "for idx in range(len(test_jsons)):\n",
    "    try:\n",
    "        annotation = mmcv.load(test_jsons_dir + test_jsons[idx])\n",
    "\n",
    "        filename = annotation['imagePath']\n",
    "        # print(filename)\n",
    "        # img_path = osp.join(image_prefix, filename)\n",
    "        # print(img_path)\n",
    "        height, width = int(annotation['imageHeight']), int(annotation['imageWidth'])\n",
    "        # print(height, width)\n",
    "\n",
    "        images.append(dict(\n",
    "        id=int(idx),\n",
    "        file_name=filename,\n",
    "        height=height,\n",
    "        width=width))\n",
    "\n",
    "        bboxes = []\n",
    "        labels = []\n",
    "        masks = []\n",
    "        if len(annotation['shapes']) > 0:\n",
    "            for obj in annotation['shapes']:\n",
    "                px = [points[0] for points in obj['points']]\n",
    "                py = [points[1] for points in obj['points']]\n",
    "                poly = [(x + 0.5, y + 0.5) for x, y in zip(px, py)]\n",
    "                poly = [p for x in poly for p in x]\n",
    "\n",
    "                x_min, y_min, x_max, y_max = (\n",
    "                    min(px), min(py), max(px), max(py))\n",
    "\n",
    "                data_anno = dict(\n",
    "                    image_id=idx,\n",
    "                    id=obj_count,\n",
    "                    category_id=0,\n",
    "                    bbox=[x_min, y_min, x_max - x_min, y_max - y_min],\n",
    "                    area=(x_max - x_min) * (y_max - y_min),\n",
    "                    segmentation=[poly],\n",
    "                    iscrowd=0)\n",
    "                annotations.append(data_anno)\n",
    "                obj_count += 1    \n",
    "        # else:    \n",
    "            # No annotations associated in this case\n",
    "            \n",
    "            # data_anno = dict(\n",
    "            #     image_id=idx,\n",
    "            #     id='',\n",
    "            #     category_id=0,\n",
    "            #     bbox=[],\n",
    "            #     area=0,\n",
    "            #     segmentation=[],\n",
    "            #     iscrowd=0)\n",
    "            # annotations.append(data_anno)\n",
    "            # obj_count += 1    \n",
    "\n",
    "    except: \n",
    "        print(test_jsons[idx])\n",
    "    \n",
    "out_file = DATA_DIR + 'test/test_annotation_coco.json'\n",
    "coco_format_json = dict(\n",
    "    images=images,\n",
    "    annotations=annotations,\n",
    "    categories=[{'id':0, 'name': 'shiptrack'}])\n",
    "\n",
    "mmcv.dump(coco_format_json, out_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Debugging why Positives filter didn't work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset\n",
    "import pandas as pd\n",
    "import string\n",
    "import shutil\n",
    "from shutil import copyfile\n",
    "import mmcv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '/deep/group/aicc-bootcamp/cloud-pollution/data/combined_v3_typed_new_composite/COCO_format_cropped_postrain/'\n",
    "train_path = DATA_DIR + 'train.csv'\n",
    "val_path = DATA_DIR + 'val.csv'\n",
    "test_path = DATA_DIR + 'test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = mmcv.load('/deep/group/aicc-bootcamp/cloud-pollution/data/combined_v3_typed_new_composite/COCO_format_cropped_postrain/train/train_annotation_coco.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1861"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images = test['images']\n",
    "len(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1861\n"
     ]
    }
   ],
   "source": [
    "train_csv = pd.read_csv(train_path)\n",
    "\n",
    "i = 0\n",
    "\n",
    "for idx in range(len(train_csv)):\n",
    "    if train_csv.loc[idx, 'contains_shiptrack'] == True:\n",
    "        i+=1\n",
    "\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = test['annotations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5121"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_ids = []\n",
    "\n",
    "for i in range(len(annotations)):\n",
    "    image_ids.append(annotations[i]['image_id'])\n",
    "\n",
    "len(image_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1833"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_ids = set(image_ids)\n",
    "len(set_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1861"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "excluded = []\n",
    "true_image_ids = []\n",
    "\n",
    "for i in range(len(images)):\n",
    "    true_image_ids.append(images[i]['id'])\n",
    "\n",
    "len(true_image_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_ids = list(set_ids)\n",
    "\n",
    "for id in true_image_ids:\n",
    "    if id not in list_ids:\n",
    "        excluded.append(id)\n",
    "\n",
    "len(excluded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[59,\n",
       " 177,\n",
       " 194,\n",
       " 239,\n",
       " 307,\n",
       " 351,\n",
       " 363,\n",
       " 412,\n",
       " 589,\n",
       " 596,\n",
       " 650,\n",
       " 871,\n",
       " 909,\n",
       " 923,\n",
       " 972,\n",
       " 1012,\n",
       " 1092,\n",
       " 1166,\n",
       " 1182,\n",
       " 1222,\n",
       " 1251,\n",
       " 1430,\n",
       " 1446,\n",
       " 1500,\n",
       " 1640,\n",
       " 1676,\n",
       " 1757,\n",
       " 1821]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "excluded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "test_mask = np.load('/deep/group/aicc-bootcamp/cloud-pollution/data/combined_v3_typed_new_composite/COCO_format_cropped_postrain/train/masks/myd2008249.1325D_crop_1.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1015, 1354)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92310"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(test_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageDraw\n",
    "from PIL import UnidentifiedImageError\n",
    "from collections import defaultdict\n",
    "from shapely.geometry import asLineString, asMultiLineString\n",
    "from shapely.ops import split\n",
    "\n",
    "DATA_DIR = '/deep/group/aicc-bootcamp/cloud-pollution/data/combined_v3_typed_new_composite/'\n",
    "JSONS_PATH = '/deep/group/aicc-bootcamp/cloud-pollution/data/combined_v3_typed_new_composite/highres_jsons_cropped_updated/'\n",
    "img_path = DATA_DIR + 'images_only/'\n",
    "pts_path = DATA_DIR + 'points_only/myd2008249.1325D.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Loop over all points files here\n",
    "# for txt in pts_path:\n",
    "with open(os.path.join(pts_path)) as f:\n",
    "    points_list = []\n",
    "    try:\n",
    "        for line in f:\n",
    "            ln = line.strip().split()\n",
    "            points = [ln[0]] + [int(x) for x in ln[1:]]\n",
    "            points_list.append(points)\n",
    "    except UnicodeDecodeError:\n",
    "        print('UnicodeDecodeError: ', str(os.path.join(pts_path)))\n",
    "\n",
    "# Loop over all elements in points list here\n",
    "\n",
    "EPS = 1e-6\n",
    "coords = points_list\n",
    "shapes_0 = []\n",
    "shapes_1 = []\n",
    "for coord in coords:\n",
    "    #print(coord)\n",
    "    # col = colors[coord[0]] # TODO: KeyError: 'EDG'??\n",
    "    # col = int((i/len(coords))*255)\n",
    "    coord_y = [coord[i:i + 1][0] for i in range(2, len(coord), 2)]\n",
    "    coord_x = [coord[i:i + 1][0] for i in range(1, len(coord), 2)]\n",
    "    smoothnes_raw_x = (np.square(np.std(np.diff(coord_x)))) / (np.abs(np.mean(np.diff(coord_x))) + EPS)\n",
    "    smoothnes_raw_y = (np.square(np.std(np.diff(coord_y)))) / (np.abs(np.mean(np.diff(coord_y))) + EPS)\n",
    "    smoothness_raw_avg = 0.5*(smoothnes_raw_x + smoothnes_raw_y)\n",
    "\n",
    "    coord_proc = [tuple(coord[i:i + 2]) for i in range(1, len(coord), 2)]\n",
    "    coord_proc.sort(key=lambda y: y[0])\n",
    "    coord_proc= [item for t in coord_proc for item in t]\n",
    "    coord_proc_y = [coord_proc[i:i + 1][0] for i in range(1, len(coord_proc), 2)]\n",
    "    coord_proc_x = [coord_proc[i:i + 1][0] for i in range(0, len(coord_proc), 2)]\n",
    "    smoothnes_sorted_x = (np.square(np.std(np.diff(coord_proc_x)))) / (np.abs(np.mean(np.diff(coord_proc_x))) + EPS)\n",
    "    smoothnes_sorted_y = (np.square(np.std(np.diff(coord_proc_y)))) / (np.abs(np.mean(np.diff(coord_proc_y))) + EPS)\n",
    "    smoothness_sorted_avg = 0.5*(smoothnes_sorted_x + smoothnes_sorted_y)\n",
    "\n",
    "    coord_proc_2 = [tuple(coord[i:i + 2]) for i in range(1, len(coord), 2)]\n",
    "    coord_proc_2.sort(key=lambda y: y[1])\n",
    "    coord_proc_2= [item for t in coord_proc_2 for item in t]\n",
    "    coord_proc_2_y = [coord_proc_2[i:i + 1][0] for i in range(1, len(coord_proc_2), 2)]\n",
    "    coord_proc_2_x = [coord_proc_2[i:i + 1][0] for i in range(0, len(coord_proc_2), 2)]\n",
    "    smoothnes_sorted_2_x = (np.square(np.std(np.diff(coord_proc_2_x)))) / (np.abs(np.mean(np.diff(coord_proc_2_x))) + EPS)\n",
    "    smoothnes_sorted_2_y = (np.square(np.std(np.diff(coord_proc_2_y)))) / (np.abs(np.mean(np.diff(coord_proc_2_y))) + EPS)\n",
    "    smoothness_sorted_2_avg = 0.5*(smoothnes_sorted_2_x + smoothnes_sorted_2_y)\n",
    "    \n",
    "    min_jaggedness = min([smoothness_raw_avg, smoothness_sorted_avg, smoothness_sorted_2_avg])\n",
    "\n",
    "    if smoothness_sorted_avg == min_jaggedness:\n",
    "        final_points_x = coord_proc_x\n",
    "        final_points_y = coord_proc_y\n",
    "        # draw.line([c + padding for c in coord_proc], fill=col, width=width)\n",
    "    elif smoothness_sorted_2_avg == min_jaggedness:\n",
    "        final_points_x = coord_proc_2_x\n",
    "        final_points_y = coord_proc_2_y\n",
    "    else:\n",
    "        final_points_x = coord_x\n",
    "        final_points_y = coord_y\n",
    "\n",
    "    tuples = [(final_points_x[i], 2030 - final_points_y[i]) for i in range(len(final_points_x))]\n",
    "    # tuples = [(coord_proc_x[i], 2030 - coord_proc_y[i]) for i in range(len(final_points_x))]\n",
    "\n",
    "    points = asLineString(tuples)\n",
    "    split_line = asLineString([(0, 1015), (1354, 1015)])\n",
    "    result = split(points, split_line)\n",
    "    for i in range(len(result)):\n",
    "        if result[i].bounds[1] == 1015.0:\n",
    "            poly_0 = result[i].buffer(5)\n",
    "            shapes_0.append({\"label\": \"shiptrack\",\n",
    "                            \"points\": list(poly_0.exterior.coords),\n",
    "                            \"group_id\": None,\n",
    "                            \"shape_type\": \"polygon\",\n",
    "                            \"flags\": {}\n",
    "                            })\n",
    "        else: \n",
    "            poly_1 = result[i].buffer(5)\n",
    "            shapes_1.append({\"label\": \"shiptrack\",\n",
    "                \"points\": list(poly_1.exterior.coords),\n",
    "                \"group_id\": None,\n",
    "                \"shape_type\": \"polygon\",\n",
    "                \"flags\": {}\n",
    "                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"273.24\" height=\"163.2399999999999\" viewBox=\"427.88 973.88 273.24 163.2399999999999\" preserveAspectRatio=\"xMinYMin meet\"><g transform=\"matrix(1,0,0,-1,0,2111.0)\"><polyline fill=\"none\" stroke=\"#66cc99\" stroke-width=\"2.0\" points=\"438.0,1127.0 466.0,1117.0 487.0,1108.0 515.0,1101.0 549.0,1093.0 580.0,1084.0 618.0,1080.0 655.0,1070.0 688.0,1060.0 688.0,984.0 691.0,1005.0 691.0,1032.0\" opacity=\"0.8\" /></g></svg>"
      ],
      "text/plain": [
       "<shapely.geometry.linestring.LineStringAdapter at 0x7f0e14bccd60>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"100.0\" height=\"100.0\" viewBox=\"690.32 1014.32 1.3599999999999 18.360000000000014\" preserveAspectRatio=\"xMinYMin meet\"><g transform=\"matrix(1,0,0,-1,0,2047.0)\"><polyline fill=\"none\" stroke=\"#66cc99\" stroke-width=\"0.36720000000000025\" points=\"691.0,1015.0 691.0,1032.0\" opacity=\"0.8\" /></g></svg>"
      ],
      "text/plain": [
       "<shapely.geometry.linestring.LineString at 0x7f64753c60a0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(shapes_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(shapes_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(shapes_0[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"273.24\" height=\"163.2399999999999\" viewBox=\"427.88 973.88 273.24 163.2399999999999\" preserveAspectRatio=\"xMinYMin meet\"><g transform=\"matrix(1,0,0,-1,0,2111.0)\"><polyline fill=\"none\" stroke=\"#66cc99\" stroke-width=\"2.0\" points=\"438.0,1127.0 466.0,1117.0 487.0,1108.0 515.0,1101.0 549.0,1093.0 580.0,1084.0 618.0,1080.0 655.0,1070.0 688.0,1060.0 691.0,1032.0 691.0,1005.0 688.0,984.0\" opacity=\"0.8\" /></g></svg>"
      ],
      "text/plain": [
       "<shapely.geometry.linestring.LineStringAdapter at 0x7faafd15f6d0>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuples = [(coord_proc_2_x[i], 2030 - coord_proc_2_y[i]) for i in range(len(coord_proc_2_x))]\n",
    "points = asLineString(tuples)\n",
    "points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"273.24\" height=\"163.2399999999999\" viewBox=\"427.88 973.88 273.24 163.2399999999999\" preserveAspectRatio=\"xMinYMin meet\"><g transform=\"matrix(1,0,0,-1,0,2111.0)\"><polyline fill=\"none\" stroke=\"#66cc99\" stroke-width=\"2.0\" points=\"438.0,1127.0 466.0,1117.0 487.0,1108.0 515.0,1101.0 549.0,1093.0 580.0,1084.0 618.0,1080.0 655.0,1070.0 688.0,1060.0 688.0,984.0 691.0,1005.0 691.0,1032.0\" opacity=\"0.8\" /></g></svg>"
      ],
      "text/plain": [
       "<shapely.geometry.linestring.LineStringAdapter at 0x7faafd319f40>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_points_y_test = [2030 - y for y in final_points_y]\n",
    "tuples_test = [(final_points_x[i], final_points_y_test[i]) for i in range(len(final_points_x))]\n",
    "points_test = asLineString(tuples_test)\n",
    "points_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"300\" height=\"165.4000000000001\" viewBox=\"408.8 972.8 302.40000000000003 165.4000000000001\" preserveAspectRatio=\"xMinYMin meet\"><g transform=\"matrix(1,0,0,-1,0,2111.0)\"><g><polyline fill=\"none\" stroke=\"#66cc99\" stroke-width=\"2.016\" points=\"438.0,1127.0 466.0,1117.0 487.0,1108.0 515.0,1101.0 549.0,1093.0 580.0,1084.0 618.0,1080.0 655.0,1070.0 688.0,1060.0 688.0,984.0 691.0,1005.0 691.0,1032.0\" opacity=\"0.8\" /><polyline fill=\"none\" stroke=\"#66cc99\" stroke-width=\"2.016\" points=\"420.0,1015.0 700.0,1015.0\" opacity=\"0.8\" /></g></g></svg>"
      ],
      "text/plain": [
       "<shapely.geometry.multilinestring.MultiLineStringAdapter at 0x7faafd12d7f0>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_line = asLineString([(420, 1015), (700, 1015)])\n",
    "merge = asMultiLineString([points_test, split_line])\n",
    "merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"273.24\" height=\"132.2399999999999\" viewBox=\"427.88 1004.88 273.24 132.2399999999999\" preserveAspectRatio=\"xMinYMin meet\"><g transform=\"matrix(1,0,0,-1,0,2142.0)\"><g><polyline fill=\"none\" stroke=\"#66cc99\" stroke-width=\"2.0\" points=\"438.0,1127.0 466.0,1117.0 487.0,1108.0 515.0,1101.0 549.0,1093.0 580.0,1084.0 618.0,1080.0 655.0,1070.0 688.0,1060.0 688.0,1015.0\" opacity=\"0.8\" /><polyline fill=\"none\" stroke=\"#66cc99\" stroke-width=\"2.0\" points=\"691.0,1015.0 691.0,1032.0\" opacity=\"0.8\" /></g></g></svg>"
      ],
      "text/plain": [
       "<shapely.geometry.multilinestring.MultiLineStringAdapter at 0x7faafd29dd90>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_line = asLineString([(0, 1015), (1354, 1015)])\n",
    "results_test = split(points_test, split_line)\n",
    "merged_split = asMultiLineString([results_test[0], results_test[2]])\n",
    "merged_split\n",
    "merged_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"284.0336502669335\" height=\"143.0336502669336\" viewBox=\"422.4861145577678 999.4802351752987 284.0336502669335 143.0336502669336\" preserveAspectRatio=\"xMinYMin meet\"><g transform=\"matrix(1,0,0,-1,0,2141.994120617531)\"><path fill-rule=\"evenodd\" fill=\"#66cc99\" stroke=\"#555555\" stroke-width=\"2.0\" opacity=\"0.6\" d=\"M 467.6816819849908,1121.708709557974 L 467.96959649289585,1121.5957251500904 L 488.60135304413325,1112.7535437709887 L 516.1789942460372,1105.8591334705127 L 550.1451966686277,1097.8670858416679 L 550.3940509743197,1097.801731133768 L 580.9663441499447,1088.92590408278 L 618.5234239225903,1084.972527264607 L 618.9168836257847,1084.9152135677678 L 619.3045451257138,1084.8268169651412 L 656.3045451257138,1074.8268169651412 L 656.4500369764144,1074.7851220221673 L 689.4500369764144,1064.7851220221673 L 689.9121815985594,1064.6199092560494 L 690.355902510871,1064.4101840504986 L 690.7769244994564,1064.1579670903361 L 691.1711910526553,1063.8656884648867 L 691.5349034451905,1063.5361642542705 L 691.8645573385681,1063.1725693967066 L 692.1569765450864,1062.778407098249 L 692.4093436301383,1062.35747507969 L 692.6192270579586,1061.9138289858354 L 692.7846046192675,1061.4517433097087 L 692.9038829150884,1060.9756702081665 L 692.9759127090134,1060.4901966057398 L 693.0,1060.0 L 693.0,1036.5784470299031 L 693.35698368413,1036.4096063217419 L 693.777851165098,1036.1573480615127 L 694.1719664208182,1035.8650522668138 L 694.5355339059328,1035.5355339059327 L 694.8650522668137,1035.1719664208183 L 695.1573480615127,1034.7778511650981 L 695.4096063217418,1034.35698368413 L 695.6193976625565,1033.9134171618255 L 695.784701678661,1033.4514233862724 L 695.9039264020162,1032.9754516100807 L 695.9759236333609,1032.4900857016478 L 696.0,1032.0 L 696.0,1015.0 L 695.9759236333609,1014.5099142983522 L 695.9039264020162,1014.0245483899193 L 695.784701678661,1013.5485766137277 L 695.6193976625565,1013.0865828381745 L 695.4096063217418,1012.64301631587 L 695.1573480615127,1012.222148834902 L 694.8650522668137,1011.8280335791818 L 694.5355339059328,1011.4644660940672 L 694.1719664208182,1011.1349477331863 L 693.777851165098,1010.8426519384873 L 693.35698368413,1010.5903936782582 L 692.9134171618255,1010.3806023374435 L 692.4514233862723,1010.215298321339 L 691.9754516100807,1010.0960735979838 L 691.4900857016478,1010.0240763666391 L 691.0,1010.0 L 690.5099142983522,1010.0240763666391 L 690.0245483899193,1010.0960735979838 L 689.5485766137277,1010.215298321339 L 689.5,1010.2326793116529 L 689.4514233862723,1010.215298321339 L 688.9754516100807,1010.0960735979838 L 688.4900857016478,1010.0240763666391 L 688.0,1010.0 L 687.5099142983522,1010.0240763666391 L 687.0245483899193,1010.0960735979838 L 686.5485766137277,1010.215298321339 L 686.0865828381745,1010.3806023374435 L 685.64301631587,1010.5903936782582 L 685.222148834902,1010.8426519384873 L 684.8280335791818,1011.1349477331863 L 684.4644660940672,1011.4644660940672 L 684.1349477331863,1011.8280335791818 L 683.8426519384873,1012.222148834902 L 683.5903936782582,1012.64301631587 L 683.3806023374435,1013.0865828381745 L 683.215298321339,1013.5485766137276 L 683.0960735979838,1014.0245483899193 L 683.0240763666391,1014.5099142983522 L 683.0,1015.0 L 683.0,1056.2906243486161 L 653.6223933895843,1065.1929293820754 L 617.0802080768141,1075.069195682824 L 579.4765760774097,1079.027472735393 L 579.0374667150444,1079.0935216626024 L 578.6059490256803,1079.198268866232 L 547.7295351345558,1088.1623890281712 L 513.8548033313723,1096.1329141583321 L 513.7873218748183,1096.1492874992734 L 485.78732187481836,1103.1492874992734 L 485.4037629958364,1103.261642961264 L 485.03040350710415,1103.4042748499096 L 464.17261267795055,1112.343328062404 L 436.3183180150092,1122.291290442026 L 435.8648815279023,1122.4787978246811 L 435.4320073988562,1122.7098468558254 L 435.0238644423666,1122.9822124069844 L 434.6443832982182,1123.2932714490103 L 434.29721857727463,1123.640028313286 L 433.9857136655782,1124.019143541652 L 433.7128685257143,1124.4269660472141 L 433.4813108055323,1124.8595682763048 L 433.29327053245976,1125.3127840329737 L 433.15055863711973,1125.7822486017312 L 433.0545495130785,1126.2634407821445 L 433.00616778068456,1126.7517264304702 L 433.005879382469,1127.2424030889904 L 433.05368709586435,1127.7307452732528 L 433.1491305064562,1128.2120499810676 L 433.2912904420258,1128.6816819849907 L 433.47879782468124,1129.1351184720977 L 433.7098468558255,1129.567992601144 L 433.9822124069844,1129.9761355576334 L 434.2932714490102,1130.3556167017819 L 434.64002831328594,1130.7027814227254 L 435.0191435416521,1131.0142863344217 L 435.4269660472141,1131.2871314742856 L 435.8595682763048,1131.5186891944677 L 436.3127840329737,1131.7067294675403 L 436.7822486017311,1131.8494413628803 L 437.2634407821445,1131.9454504869216 L 437.75172643047006,1131.9938322193154 L 438.24240308899033,1131.994120617531 L 438.73074527325264,1131.9463129041358 L 439.21204998106754,1131.8508694935438 L 439.6816819849908,1131.708709557974 L 467.6816819849908,1121.708709557974 z\" /></g></svg>"
      ],
      "text/plain": [
       "<shapely.geometry.polygon.Polygon at 0x7faafd1394c0>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_split.buffer(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(438.0, 1015.0, 688.0, 1127.0)\n",
      "(688.0, 984.0, 691.0, 1015.0)\n",
      "(691.0, 1015.0, 691.0, 1032.0)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(results_test)):\n",
    "    print(results_test[i].bounds)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"274.3174601067734\" height=\"136.31746010677352\" viewBox=\"425.84244582310714 1002.8400940701194 274.3174601067734 136.31746010677352\" preserveAspectRatio=\"xMinYMin meet\"><g transform=\"matrix(1,0,0,-1,0,2141.9976482470124)\"><path fill-rule=\"evenodd\" fill=\"#66cc99\" stroke=\"#555555\" stroke-width=\"2.0\" opacity=\"0.6\" d=\"M 466.6726727939963,1118.8834838231896 L 466.7878385971583,1118.8382900600361 L 487.64054121765247,1109.9014175083958 L 515.4715976984269,1102.9436533882022 L 549.4580786674511,1094.946834336667 L 549.557620389728,1094.9206924535072 L 580.3865376599773,1085.9703616331121 L 618.2093695690361,1081.9890109058429 L 618.3667534503139,1081.9660854271071 L 618.5218180502856,1081.9307267860565 L 655.5218180502856,1071.9307267860565 L 655.5800147905658,1071.914048808867 L 688.5800147905658,1061.914048808867 L 688.7648726394237,1061.8479637024197 L 688.9423610043484,1061.7640736201995 L 689.1107697997826,1061.6631868361344 L 689.2684764210621,1061.5462753859547 L 689.4139613780762,1061.4144657017082 L 689.5458229354273,1061.2690277586826 L 689.6627906180346,1061.1113628392998 L 689.7637374520554,1060.942990031876 L 689.8476908231834,1060.765531594334 L 689.913841847707,1060.5806973238834 L 689.9615531660354,1060.3902680832666 L 689.9903650836054,1060.196078642296 L 690.0,1060.0 L 690.0,1015.0 L 689.9903694533444,1014.8039657193409 L 689.9615705608064,1014.6098193559677 L 689.9138806714644,1014.419430645491 L 689.8477590650226,1014.2346331352699 L 689.7638425286967,1014.057206526348 L 689.6629392246051,1013.8888595339608 L 689.5460209067255,1013.7312134316727 L 689.4142135623731,1013.5857864376269 L 689.2687865683273,1013.4539790932745 L 689.1111404660392,1013.3370607753949 L 688.942793473652,1013.2361574713033 L 688.7653668647301,1013.1522409349774 L 688.580569354509,1013.0861193285356 L 688.3901806440323,1013.0384294391936 L 688.1960342806591,1013.0096305466556 L 688.0,1013.0 L 687.8039657193409,1013.0096305466556 L 687.6098193559677,1013.0384294391936 L 687.419430645491,1013.0861193285356 L 687.2346331352699,1013.1522409349774 L 687.057206526348,1013.2361574713033 L 686.8888595339608,1013.3370607753949 L 686.7312134316727,1013.4539790932745 L 686.5857864376269,1013.5857864376269 L 686.4539790932745,1013.7312134316727 L 686.3370607753949,1013.8888595339608 L 686.2361574713033,1014.057206526348 L 686.1522409349774,1014.2346331352699 L 686.0861193285356,1014.419430645491 L 686.0384294391936,1014.6098193559677 L 686.0096305466556,1014.8039657193409 L 686.0,1015.0 L 686.0,1058.5162497394465 L 654.4489573558351,1068.0771717528298 L 617.6320832307258,1078.0276782731296 L 579.7906304309639,1082.0109890941571 L 579.6149866860177,1082.0374086650409 L 579.442379610272,1082.0793075464928 L 548.4918140538213,1091.064955611269 L 514.5419213325489,1099.053165663333 L 514.5149287499273,1099.0597149997093 L 486.5149287499273,1106.0597149997093 L 486.36150519833456,1106.1046571845056 L 486.2121614028417,1106.1617099399639 L 465.2690450711808,1115.1373312249614 L 437.3273272060037,1125.1165161768104 L 437.1459526111609,1125.1915191298724 L 436.97280295954243,1125.2839387423303 L 436.8095457769466,1125.3928849627937 L 436.6577533192873,1125.517308579604 L 436.51888743090984,1125.6560113253145 L 436.3942854662313,1125.8076574166607 L 436.28514741028573,1125.9707864188856 L 436.19252432221293,1126.1438273105218 L 436.1173082129839,1126.3251136131894 L 436.0602234548479,1126.5128994406925 L 436.0218198052314,1126.7053763128579 L 436.00246711227385,1126.900690572188 L 436.00235175298764,1127.0969612355962 L 436.02147483834574,1127.292298109301 L 436.05965220258247,1127.484819992427 L 436.1165161768103,1127.6726727939963 L 436.1915191298725,1127.854047388839 L 436.2839387423302,1128.0271970404576 L 436.39288496279374,1128.1904542230534 L 436.51730857960405,1128.3422466807128 L 436.65601132531435,1128.48111256909 L 436.80765741666085,1128.6057145337688 L 436.9707864188856,1128.7148525897142 L 437.1438273105219,1128.8074756777871 L 437.32511361318944,1128.882691787016 L 437.5128994406924,1128.9397765451522 L 437.7053763128578,1128.9781801947686 L 437.900690572188,1128.9975328877263 L 438.09696123559615,1128.9976482470124 L 438.29229810930104,1128.9785251616543 L 438.484819992427,1128.9403477974176 L 438.6726727939963,1128.8834838231896 L 466.6726727939963,1118.8834838231896 z\" /></g></svg>"
      ],
      "text/plain": [
       "<shapely.geometry.polygon.Polygon at 0x7faafd0d11c0>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test[0].buffer(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABUoAAAfuCAAAAAAPxlyZAAANWklEQVR4nO3dYVKjUBCFUXT/e878sCx1NAlwCa/7cc4KqHrd35AJhmUBAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAALa7jb4A4Ju30RfAPrfF4UEdtrGnz5tS5wclvI++ACI3H/ShAnc1Pf0oqEOE0WxhS7/uRZ0jDGUFW/rrY72jhHHsX0t3/ofUacIglq+j+981OU8Ywup19PBre0cK57N3TakpVGLp+npUU+cKp7Jynbk1hSLsW3NuTaEC29bfg5o6XjiHXZuCmsJYFm0WnjWFgazZPMQUhrFkU1FTGMOGTUZMYQT7NR1fQcH5LNeMPGwKJ7NYc3r8nhKnDgezVLN69tInJw8HslATe/oKPacPB7FMU1vxPlITAAewSJNb9XJnUwAhSzS/VTU1CZCwQFewLqamAXazPJehp/A69uZS5BRew85cjpzC8ezLJckpHMuuXNbanJoReM6aXJqcwjGsyOX5sA85+8Eip5CyG3zyV1Gwm8XgOzmFXSwF//MLKLCZjeAvcgqb2Abu8munsJZV4CE5hTWsAU89z6kx4ursAGt48BQeMvqstfqvTJdlMVlcjIFng201/WDEuAJzzkZ7cvrBsDEv0812+2tq4JiUyWYfN6fwjbFmNzen8MlIE5FTWBbjTE5NwSxzJA9LcVXmmFfY2FRjSHdmmNfyJj4uwfxyjjVJNY20ZXg5j5wyLYPLqdSUOZlaTienzMfEMoRf52cuxpVh/Do/8zCrjKSmTMKgMpqcMgFDSgVPc2pQqc2EUoSa0pnxpBA5pSujSS1qSkvmknoe5tTIUpG5pKT7NTWyVGQuqervmppYSjKYVPY7p2/LstyMLdWYSYr76+bU2FKNmaS+XzU1tlRjJmnhZ02NLdWYSdr4yqmxpZr30RcA2+15STS8kpTShntR6pJSgJiUAsSkFCAmpQAxKQWISSlATEoBYlIKEJNS+vh6Rt+fO1GMlALEpBQgJqUAMSkFiEkpQExKAWJSChCTUoCYlNKSZ/SpRUppxCtJqEpKAWJSChCTUoCYlALEpBQgJqUAMSkFiEkpQExKAWJSChCTUoCYlALEpBQgJqUAMSkFiEkpQExKAWJSChCTUoCYlALEpBQgJqUAMSkFiEkpQExKAWJSChCTUoCYlALEpBQgJqUAMSkFiEkpQExKaeQ2+gLgDikFiEkpLb2NvgD4QUoBYlIKEJNSgJiUAsSklD48C0VZUgoQk1I68iwUxUgpQExKAWJSChCTUoCYlNKGZ6GoS0oBYlJKQ56FohopBYj5551ebstibAFyvn8CAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAIDL+gfGBFbYsrQkYwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=1354x2030 at 0x7FAAFD2A3430>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_shape=(1354,2030)\n",
    "padding = 0\n",
    "img = Image.new('L', old_shape, 0)\n",
    "draw = ImageDraw.Draw(img)\n",
    "\n",
    "draw.line([c + padding for c in coord_proc_2], fill=255, width=10)\n",
    "\n",
    "processed_msk = np.abs(np.array(img).astype(int)).astype(np.uint8)\n",
    "Image.fromarray(processed_msk)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Coco Format with Smooth and Debugged Masks and Jsons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nothing done.\n"
     ]
    }
   ],
   "source": [
    "%reset\n",
    "import pandas as pd\n",
    "import string\n",
    "import shutil\n",
    "from shutil import copyfile\n",
    "import mmcv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '/deep/group/aicc-bootcamp/cloud-pollution/data/combined_v3_typed_new_composite/COCO_smooth_cropped/'\n",
    "train_path = DATA_DIR + 'train.csv'\n",
    "val_path = DATA_DIR + 'val.csv'\n",
    "test_path = DATA_DIR + 'test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv = pd.read_csv(train_path)\n",
    "\n",
    "train_img_dir = DATA_DIR + 'train/images/'\n",
    "\n",
    "for item in train_csv.loc[:,'image']:\n",
    "    image_name = item.split(\"/\")[-1]\n",
    "    shutil.copyfile(item, train_img_dir + image_name)\n",
    "\n",
    "train_jsons_dir = DATA_DIR + 'train/jsons/'\n",
    "\n",
    "for item in train_csv.loc[:,'json']:\n",
    "    json_name = item.split(\"/\")[-1]\n",
    "    shutil.copyfile(item, train_jsons_dir + json_name)\n",
    "\n",
    "train_masks_dir = DATA_DIR + 'train/masks/'\n",
    "\n",
    "for item in train_csv.loc[:,'mask']:\n",
    "    mask_name = item.split(\"/\")[-1]\n",
    "    shutil.copyfile(item, train_masks_dir + mask_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_csv = pd.read_csv(val_path)\n",
    "\n",
    "val_img_dir = DATA_DIR + 'val/images/'\n",
    "\n",
    "for item in val_csv.loc[:,'image']:\n",
    "    image_name = item.split(\"/\")[-1]\n",
    "    shutil.copyfile(item, val_img_dir + image_name)\n",
    "\n",
    "val_jsons_dir = DATA_DIR + 'val/jsons/'\n",
    "\n",
    "for item in val_csv.loc[:,'json']:\n",
    "    json_name = item.split(\"/\")[-1]\n",
    "    shutil.copyfile(item, val_jsons_dir + json_name)\n",
    "\n",
    "val_masks_dir = DATA_DIR + 'val/masks/'\n",
    "\n",
    "for item in val_csv.loc[:,'mask']:\n",
    "    mask_name = item.split(\"/\")[-1]\n",
    "    shutil.copyfile(item, val_masks_dir + mask_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_csv = pd.read_csv(test_path)\n",
    "\n",
    "test_img_dir = DATA_DIR + 'test/images/'\n",
    "\n",
    "for item in test_csv.loc[:,'image']:\n",
    "    image_name = item.split(\"/\")[-1]\n",
    "    shutil.copyfile(item, test_img_dir + image_name)\n",
    "\n",
    "test_jsons_dir = DATA_DIR + 'test/jsons/'\n",
    "\n",
    "for item in test_csv.loc[:,'json']:\n",
    "    json_name = item.split(\"/\")[-1]\n",
    "    shutil.copyfile(item, test_jsons_dir + json_name)\n",
    "\n",
    "test_masks_dir = DATA_DIR + 'test/masks/'\n",
    "\n",
    "for item in test_csv.loc[:,'mask']:\n",
    "    mask_name = item.split(\"/\")[-1]\n",
    "    shutil.copyfile(item, test_masks_dir + mask_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_jsons_dir = DATA_DIR + 'train/jsons/'\n",
    "\n",
    "# Getting all train json files\n",
    "train_jsons = [f for f in os.listdir(train_jsons_dir)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "435"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(1):\n",
    "    annotation = mmcv.load(train_jsons_dir + train_jsons[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[109.33311228437084, 831.4948222243347]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotation['shapes'][0]['points'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "myd2016355.1605D_crop_0.json\n",
      "myd2016355.1605D_crop_1.json\n"
     ]
    }
   ],
   "source": [
    "# Train Annotations\n",
    "\n",
    "annotations = []\n",
    "images = []\n",
    "obj_count = 0\n",
    "for idx in range(len(train_jsons)):\n",
    "    try:\n",
    "        annotation = mmcv.load(train_jsons_dir + train_jsons[idx])\n",
    "\n",
    "        filename = annotation['imagePath']\n",
    "        # print(filename)\n",
    "        # img_path = osp.join(image_prefix, filename)\n",
    "        # print(img_path)\n",
    "        height, width = int(annotation['imageHeight']), int(annotation['imageWidth'])\n",
    "        # print(height, width)\n",
    "\n",
    "        images.append(dict(\n",
    "        id=int(idx),\n",
    "        file_name=filename,\n",
    "        height=height,\n",
    "        width=width))\n",
    "\n",
    "        bboxes = []\n",
    "        labels = []\n",
    "        masks = []\n",
    "        if len(annotation['shapes']) > 0:\n",
    "            for obj in annotation['shapes']:\n",
    "                px = [points[0] for points in obj['points']]\n",
    "                py = [points[1] for points in obj['points']]\n",
    "                poly = [(x + 0.5, y + 0.5) for x, y in zip(px, py)]\n",
    "                poly = [p for x in poly for p in x]\n",
    "\n",
    "                x_min, y_min, x_max, y_max = (\n",
    "                    min(px), min(py), max(px), max(py))\n",
    "\n",
    "                data_anno = dict(\n",
    "                    image_id=idx,\n",
    "                    id=obj_count,\n",
    "                    category_id=0,\n",
    "                    bbox=[x_min, y_min, x_max - x_min, y_max - y_min],\n",
    "                    area=(x_max - x_min) * (y_max - y_min),\n",
    "                    segmentation=[poly],\n",
    "                    iscrowd=0)\n",
    "                annotations.append(data_anno)\n",
    "                obj_count += 1    \n",
    "        # else:    \n",
    "            # No annotations associated in this case\n",
    "            \n",
    "            # data_anno = dict(\n",
    "            #     image_id=idx,\n",
    "            #     id='',\n",
    "            #     category_id=0,\n",
    "            #     bbox=[],\n",
    "            #     area=0,\n",
    "            #     segmentation=[],\n",
    "            #     iscrowd=0)\n",
    "            # annotations.append(data_anno)\n",
    "            # obj_count += 1    \n",
    "\n",
    "    except: \n",
    "        print(train_jsons[idx])\n",
    "    \n",
    "out_file = DATA_DIR + 'train/train_annotation_coco.json'\n",
    "coco_format_json = dict(\n",
    "    images=images,\n",
    "    annotations=annotations,\n",
    "    categories=[{'id':0, 'name': 'shiptrack'}])\n",
    "\n",
    "mmcv.dump(coco_format_json, out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_jsons_dir = DATA_DIR + 'val/jsons/'\n",
    "\n",
    "# Getting all val json files\n",
    "val_jsons = [f for f in os.listdir(val_jsons_dir)]\n",
    "\n",
    "# Val Annotations\n",
    "\n",
    "annotations = []\n",
    "images = []\n",
    "obj_count = 0\n",
    "for idx in range(len(val_jsons)):\n",
    "    try:\n",
    "        annotation = mmcv.load(val_jsons_dir + val_jsons[idx])\n",
    "\n",
    "        filename = annotation['imagePath']\n",
    "        # print(filename)\n",
    "        # img_path = osp.join(image_prefix, filename)\n",
    "        # print(img_path)\n",
    "        height, width = int(annotation['imageHeight']), int(annotation['imageWidth'])\n",
    "        # print(height, width)\n",
    "\n",
    "        images.append(dict(\n",
    "        id=int(idx),\n",
    "        file_name=filename,\n",
    "        height=height,\n",
    "        width=width))\n",
    "\n",
    "        bboxes = []\n",
    "        labels = []\n",
    "        masks = []\n",
    "        if len(annotation['shapes']) > 0:\n",
    "            for obj in annotation['shapes']:\n",
    "                px = [points[0] for points in obj['points']]\n",
    "                py = [points[1] for points in obj['points']]\n",
    "                poly = [(x + 0.5, y + 0.5) for x, y in zip(px, py)]\n",
    "                poly = [p for x in poly for p in x]\n",
    "\n",
    "                x_min, y_min, x_max, y_max = (\n",
    "                    min(px), min(py), max(px), max(py))\n",
    "\n",
    "                data_anno = dict(\n",
    "                    image_id=idx,\n",
    "                    id=obj_count,\n",
    "                    category_id=0,\n",
    "                    bbox=[x_min, y_min, x_max - x_min, y_max - y_min],\n",
    "                    area=(x_max - x_min) * (y_max - y_min),\n",
    "                    segmentation=[poly],\n",
    "                    iscrowd=0)\n",
    "                annotations.append(data_anno)\n",
    "                obj_count += 1    \n",
    "        # else:    \n",
    "            # No annotations associated in this case\n",
    "            \n",
    "            # data_anno = dict(\n",
    "            #     image_id=idx,\n",
    "            #     id='',\n",
    "            #     category_id=0,\n",
    "            #     bbox=[],\n",
    "            #     area=0,\n",
    "            #     segmentation=[],\n",
    "            #     iscrowd=0)\n",
    "            # annotations.append(data_anno)\n",
    "            # obj_count += 1    \n",
    "\n",
    "    except: \n",
    "        print(val_jsons[idx])\n",
    "    \n",
    "out_file = DATA_DIR + 'val/val_annotation_coco.json'\n",
    "coco_format_json = dict(\n",
    "    images=images,\n",
    "    annotations=annotations,\n",
    "    categories=[{'id':0, 'name': 'shiptrack'}])\n",
    "\n",
    "mmcv.dump(coco_format_json, out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_jsons_dir = DATA_DIR + 'test/jsons/'\n",
    "\n",
    "# Getting all test json files\n",
    "test_jsons = [f for f in os.listdir(test_jsons_dir)]\n",
    "\n",
    "# Test Annotations\n",
    "\n",
    "annotations = []\n",
    "images = []\n",
    "obj_count = 0\n",
    "for idx in range(len(test_jsons)):\n",
    "    try:\n",
    "        annotation = mmcv.load(test_jsons_dir + test_jsons[idx])\n",
    "\n",
    "        filename = annotation['imagePath']\n",
    "        # print(filename)\n",
    "        # img_path = osp.join(image_prefix, filename)\n",
    "        # print(img_path)\n",
    "        height, width = int(annotation['imageHeight']), int(annotation['imageWidth'])\n",
    "        # print(height, width)\n",
    "\n",
    "        images.append(dict(\n",
    "        id=int(idx),\n",
    "        file_name=filename,\n",
    "        height=height,\n",
    "        width=width))\n",
    "\n",
    "        bboxes = []\n",
    "        labels = []\n",
    "        masks = []\n",
    "        if len(annotation['shapes']) > 0:\n",
    "            for obj in annotation['shapes']:\n",
    "                px = [points[0] for points in obj['points']]\n",
    "                py = [points[1] for points in obj['points']]\n",
    "                poly = [(x + 0.5, y + 0.5) for x, y in zip(px, py)]\n",
    "                poly = [p for x in poly for p in x]\n",
    "\n",
    "                x_min, y_min, x_max, y_max = (\n",
    "                    min(px), min(py), max(px), max(py))\n",
    "\n",
    "                data_anno = dict(\n",
    "                    image_id=idx,\n",
    "                    id=obj_count,\n",
    "                    category_id=0,\n",
    "                    bbox=[x_min, y_min, x_max - x_min, y_max - y_min],\n",
    "                    area=(x_max - x_min) * (y_max - y_min),\n",
    "                    segmentation=[poly],\n",
    "                    iscrowd=0)\n",
    "                annotations.append(data_anno)\n",
    "                obj_count += 1    \n",
    "        # else:    \n",
    "            # No annotations associated in this case\n",
    "            \n",
    "            # data_anno = dict(\n",
    "            #     image_id=idx,\n",
    "            #     id='',\n",
    "            #     category_id=0,\n",
    "            #     bbox=[],\n",
    "            #     area=0,\n",
    "            #     segmentation=[],\n",
    "            #     iscrowd=0)\n",
    "            # annotations.append(data_anno)\n",
    "            # obj_count += 1    \n",
    "\n",
    "    except: \n",
    "        print(test_jsons[idx])\n",
    "    \n",
    "out_file = DATA_DIR + 'test/test_annotation_coco.json'\n",
    "coco_format_json = dict(\n",
    "    images=images,\n",
    "    annotations=annotations,\n",
    "    categories=[{'id':0, 'name': 'shiptrack'}])\n",
    "\n",
    "mmcv.dump(coco_format_json, out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_annotations = mmcv.load('/deep/group/aicc-bootcamp/cloud-pollution/data/combined_v3_typed_new_composite/COCO_smooth_cropped/train/train_annotation_coco.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3532"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_annotations['images'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "\n",
    "for idx in range(len(train_csv)):\n",
    "    if train_csv.loc[idx, 'contains_shiptrack'] == True:\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2132"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_ids = []\n",
    "for i in range(len(train_annotations['annotations'])):\n",
    "    image_ids.append(train_annotations['annotations'][i]['image_id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2125"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(image_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/deep/group/aicc-bootcamp/cloud-pollution/data/combined_v3_typed_new_composite/smooth_jsons_cropped/myd2009321.2255D_crop_0.json\n"
     ]
    }
   ],
   "source": [
    "for idx in range(1):\n",
    "    print(train_csv.iloc[idx,2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "JSONS_PATH = '/deep/group/aicc-bootcamp/cloud-pollution/data/combined_v3_typed_new_composite/smooth_jsons_cropped/'\n",
    "dir_sorted = os.listdir('/deep/group/aicc-bootcamp/cloud-pollution/data/combined_v3_typed_new_composite/smooth_jsons_cropped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "myd2016355.1605D_crop_1.json\n",
      "myd2016355.1605D_crop_0.json\n"
     ]
    }
   ],
   "source": [
    "for idx in range(len(dir_sorted)):\n",
    "    try:\n",
    "        mmcv.load(JSONS_PATH + dir_sorted[idx])\n",
    "    except:\n",
    "        print(dir_sorted[idx])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Coco Format with Corrected 500 Masks and Jsons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nothing done.\n"
     ]
    }
   ],
   "source": [
    "%reset\n",
    "import pandas as pd\n",
    "import string\n",
    "import shutil\n",
    "from shutil import copyfile\n",
    "import mmcv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '/deep/group/aicc-bootcamp/cloud-pollution/data/combined_v3_typed_new_composite/COCO_corrected_500/'\n",
    "train_path = DATA_DIR + 'train.csv'\n",
    "val_path = DATA_DIR + 'val.csv'\n",
    "test_path = DATA_DIR + 'test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv = pd.read_csv(train_path)\n",
    "\n",
    "train_img_dir = DATA_DIR + 'train/images/'\n",
    "\n",
    "for item in train_csv.loc[:,'image']:\n",
    "    image_name = item.split(\"/\")[-1]\n",
    "    shutil.copyfile(item, train_img_dir + image_name)\n",
    "\n",
    "# train_jsons_dir = DATA_DIR + 'train/jsons/'\n",
    "\n",
    "# for item in train_csv.loc[:,'jsons']:\n",
    "#     json_name = item.split(\"/\")[-1]\n",
    "#     shutil.copyfile(item, train_jsons_dir + json_name)\n",
    "\n",
    "# train_masks_dir = DATA_DIR + 'train/masks/'\n",
    "\n",
    "# for item in train_csv.loc[:,'mask']:\n",
    "#     mask_name = item.split(\"/\")[-1]\n",
    "#     shutil.copyfile(item, train_masks_dir + mask_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_csv = pd.read_csv(val_path)\n",
    "\n",
    "val_img_dir = DATA_DIR + 'val/images/'\n",
    "\n",
    "for item in val_csv.loc[:,'image']:\n",
    "    image_name = item.split(\"/\")[-1]\n",
    "    shutil.copyfile(item, val_img_dir + image_name)\n",
    "\n",
    "# val_jsons_dir = DATA_DIR + 'val/jsons/'\n",
    "\n",
    "# for item in val_csv.loc[:,'jsons']:\n",
    "#     json_name = item.split(\"/\")[-1]\n",
    "#     shutil.copyfile(item, val_jsons_dir + json_name)\n",
    "\n",
    "# val_masks_dir = DATA_DIR + 'val/masks/'\n",
    "\n",
    "# for item in val_csv.loc[:,'mask']:\n",
    "#     mask_name = item.split(\"/\")[-1]\n",
    "#     shutil.copyfile(item, val_masks_dir + mask_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_csv = pd.read_csv(test_path)\n",
    "\n",
    "# test_img_dir = DATA_DIR + 'test/images/'\n",
    "\n",
    "# for item in test_csv.loc[:,'image']:\n",
    "#     image_name = item.split(\"/\")[-1]\n",
    "#     shutil.copyfile(item, test_img_dir + image_name)\n",
    "\n",
    "# test_jsons_dir = DATA_DIR + 'test/jsons/'\n",
    "\n",
    "# for item in test_csv.loc[:,'jsons']:\n",
    "#     json_name = item.split(\"/\")[-1]\n",
    "#     shutil.copyfile(item, test_jsons_dir + json_name)\n",
    "\n",
    "# test_masks_dir = DATA_DIR + 'test/masks/'\n",
    "\n",
    "# for item in test_csv.loc[:,'mask']:\n",
    "#     mask_name = item.split(\"/\")[-1]\n",
    "#     shutil.copyfile(item, test_masks_dir + mask_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Annotations\n",
    "\n",
    "annotations = []\n",
    "images = []\n",
    "obj_count = 0\n",
    "for idx in range(len(train_csv.loc[:,'jsons'])):\n",
    "    try:\n",
    "        annotation = mmcv.load(train_csv.loc[idx,'jsons'])\n",
    "\n",
    "        filename = annotation['imagePath']\n",
    "        # print(filename)\n",
    "        # img_path = osp.join(image_prefix, filename)\n",
    "        # print(img_path)\n",
    "        height, width = int(annotation['imageHeight']), int(annotation['imageWidth'])\n",
    "        # print(height, width)\n",
    "\n",
    "        images.append(dict(\n",
    "        id=int(idx),\n",
    "        file_name=filename,\n",
    "        height=height,\n",
    "        width=width))\n",
    "\n",
    "        bboxes = []\n",
    "        labels = []\n",
    "        masks = []\n",
    "        if len(annotation['shapes']) > 0:\n",
    "            for obj in annotation['shapes']:\n",
    "                px = [points[0] for points in obj['points']]\n",
    "                py = [points[1] for points in obj['points']]\n",
    "                poly = [(x + 0.5, y + 0.5) for x, y in zip(px, py)]\n",
    "                poly = [p for x in poly for p in x]\n",
    "\n",
    "                x_min, y_min, x_max, y_max = (\n",
    "                    min(px), min(py), max(px), max(py))\n",
    "\n",
    "                data_anno = dict(\n",
    "                    image_id=idx,\n",
    "                    id=obj_count,\n",
    "                    category_id=0,\n",
    "                    bbox=[x_min, y_min, x_max - x_min, y_max - y_min],\n",
    "                    area=(x_max - x_min) * (y_max - y_min),\n",
    "                    segmentation=[poly],\n",
    "                    iscrowd=0)\n",
    "                annotations.append(data_anno)\n",
    "                obj_count += 1    \n",
    "        # else:    \n",
    "            # No annotations associated in this case\n",
    "            \n",
    "            # data_anno = dict(\n",
    "            #     image_id=idx,\n",
    "            #     id='',\n",
    "            #     category_id=0,\n",
    "            #     bbox=[],\n",
    "            #     area=0,\n",
    "            #     segmentation=[],\n",
    "            #     iscrowd=0)\n",
    "            # annotations.append(data_anno)\n",
    "            # obj_count += 1    \n",
    "\n",
    "    except: \n",
    "        print(train_csv.loc[idx,'jsons'])\n",
    "    \n",
    "out_file = DATA_DIR + 'train/train_annotation_coco.json'\n",
    "coco_format_json = dict(\n",
    "    images=images,\n",
    "    annotations=annotations,\n",
    "    categories=[{'id':0, 'name': 'shiptrack'}])\n",
    "\n",
    "mmcv.dump(coco_format_json, out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Val Annotations\n",
    "\n",
    "annotations = []\n",
    "images = []\n",
    "obj_count = 0\n",
    "for idx in range(len(val_csv.loc[:,'jsons'])):\n",
    "    try:\n",
    "        annotation = mmcv.load(val_csv.loc[idx,'jsons'])\n",
    "\n",
    "        filename = annotation['imagePath']\n",
    "        # print(filename)\n",
    "        # img_path = osp.join(image_prefix, filename)\n",
    "        # print(img_path)\n",
    "        height, width = int(annotation['imageHeight']), int(annotation['imageWidth'])\n",
    "        # print(height, width)\n",
    "\n",
    "        images.append(dict(\n",
    "        id=int(idx),\n",
    "        file_name=filename,\n",
    "        height=height,\n",
    "        width=width))\n",
    "\n",
    "        bboxes = []\n",
    "        labels = []\n",
    "        masks = []\n",
    "        if len(annotation['shapes']) > 0:\n",
    "            for obj in annotation['shapes']:\n",
    "                px = [points[0] for points in obj['points']]\n",
    "                py = [points[1] for points in obj['points']]\n",
    "                poly = [(x + 0.5, y + 0.5) for x, y in zip(px, py)]\n",
    "                poly = [p for x in poly for p in x]\n",
    "\n",
    "                x_min, y_min, x_max, y_max = (\n",
    "                    min(px), min(py), max(px), max(py))\n",
    "\n",
    "                data_anno = dict(\n",
    "                    image_id=idx,\n",
    "                    id=obj_count,\n",
    "                    category_id=0,\n",
    "                    bbox=[x_min, y_min, x_max - x_min, y_max - y_min],\n",
    "                    area=(x_max - x_min) * (y_max - y_min),\n",
    "                    segmentation=[poly],\n",
    "                    iscrowd=0)\n",
    "                annotations.append(data_anno)\n",
    "                obj_count += 1    \n",
    "        # else:    \n",
    "            # No annotations associated in this case\n",
    "            \n",
    "            # data_anno = dict(\n",
    "            #     image_id=idx,\n",
    "            #     id='',\n",
    "            #     category_id=0,\n",
    "            #     bbox=[],\n",
    "            #     area=0,\n",
    "            #     segmentation=[],\n",
    "            #     iscrowd=0)\n",
    "            # annotations.append(data_anno)\n",
    "            # obj_count += 1    \n",
    "\n",
    "    except: \n",
    "        print(val_csv.loc[idx,'jsons'])\n",
    "    \n",
    "out_file = DATA_DIR + 'val/val_annotation_coco.json'\n",
    "coco_format_json = dict(\n",
    "    images=images,\n",
    "    annotations=annotations,\n",
    "    categories=[{'id':0, 'name': 'shiptrack'}])\n",
    "\n",
    "mmcv.dump(coco_format_json, out_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Coco Format with Corrected ALL and NULL Masks and Jsons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nothing done.\n"
     ]
    }
   ],
   "source": [
    "%reset\n",
    "import pandas as pd\n",
    "import string\n",
    "import shutil\n",
    "from shutil import copyfile\n",
    "import mmcv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '/deep/group/aicc-bootcamp/cloud-pollution/data/combined_v3_typed_new_composite/COCO_corrected_all_w_null/'\n",
    "train_path = DATA_DIR + 'train.csv'\n",
    "val_path = DATA_DIR + 'val.csv'\n",
    "test_path = DATA_DIR + 'test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv = pd.read_csv(train_path)\n",
    "\n",
    "train_img_dir = DATA_DIR + 'train/images/'\n",
    "\n",
    "for item in train_csv.loc[:,'image']:\n",
    "    image_name = item.split(\"/\")[-1]\n",
    "    shutil.copyfile(item, train_img_dir + image_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv = pd.read_csv(train_path)\n",
    "train_jsons_dir = DATA_DIR + 'train/jsons/'\n",
    "\n",
    "for item in train_csv.loc[:,'json']:\n",
    "    json_name = item.split(\"/\")[-1]\n",
    "    shutil.copyfile(item, train_jsons_dir + json_name)\n",
    "\n",
    "# train_masks_dir = DATA_DIR + 'train/masks/'\n",
    "\n",
    "# for item in train_csv.loc[:,'mask']:\n",
    "#     mask_name = item.split(\"/\")[-1]\n",
    "#     shutil.copyfile(item, train_masks_dir + mask_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_csv = pd.read_csv(val_path)\n",
    "\n",
    "val_img_dir = DATA_DIR + 'val/images/'\n",
    "\n",
    "for item in val_csv.loc[:,'image']:\n",
    "    image_name = item.split(\"/\")[-1]\n",
    "    shutil.copyfile(item, val_img_dir + image_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_csv = pd.read_csv(val_path)\n",
    "\n",
    "val_jsons_dir = DATA_DIR + 'val/jsons/'\n",
    "\n",
    "for item in val_csv.loc[:,'json']:\n",
    "    json_name = item.split(\"/\")[-1]\n",
    "    shutil.copyfile(item, val_jsons_dir + json_name)\n",
    "\n",
    "# val_masks_dir = DATA_DIR + 'val/masks/'\n",
    "\n",
    "# for item in val_csv.loc[:,'mask']:\n",
    "#     mask_name = item.split(\"/\")[-1]\n",
    "#     shutil.copyfile(item, val_masks_dir + mask_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_csv = pd.read_csv(test_path)\n",
    "\n",
    "test_img_dir = DATA_DIR + 'test/images/'\n",
    "\n",
    "for item in test_csv.loc[:,'image']:\n",
    "    image_name = item.split(\"/\")[-1]\n",
    "    shutil.copyfile(item, test_img_dir + image_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_csv = pd.read_csv(test_path)\n",
    "\n",
    "test_jsons_dir = DATA_DIR + 'test/jsons/'\n",
    "\n",
    "for item in test_csv.loc[:,'json']:\n",
    "    json_name = item.split(\"/\")[-1]\n",
    "    shutil.copyfile(item, test_jsons_dir + json_name)\n",
    "\n",
    "# test_masks_dir = DATA_DIR + 'test/masks/'\n",
    "\n",
    "# for item in test_csv.loc[:,'mask']:\n",
    "#     mask_name = item.split(\"/\")[-1]\n",
    "#     shutil.copyfile(item, test_masks_dir + mask_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Annotations\n",
    "\n",
    "annotations = []\n",
    "images = []\n",
    "obj_count = 0\n",
    "for idx in range(len(train_csv.loc[:,'json'])):\n",
    "    try:\n",
    "        annotation = mmcv.load(train_csv.loc[idx,'json'])\n",
    "\n",
    "        filename = annotation['imagePath']\n",
    "        # print(filename)\n",
    "        # img_path = osp.join(image_prefix, filename)\n",
    "        # print(img_path)\n",
    "        height, width = int(annotation['imageHeight']), int(annotation['imageWidth'])\n",
    "        # print(height, width)\n",
    "\n",
    "        images.append(dict(\n",
    "        id=int(idx),\n",
    "        file_name=filename,\n",
    "        height=height,\n",
    "        width=width))\n",
    "\n",
    "        bboxes = []\n",
    "        labels = []\n",
    "        masks = []\n",
    "        if len(annotation['shapes']) > 0:\n",
    "            for obj in annotation['shapes']:\n",
    "                px = [points[0] for points in obj['points']]\n",
    "                py = [points[1] for points in obj['points']]\n",
    "                poly = [(x + 0.5, y + 0.5) for x, y in zip(px, py)]\n",
    "                poly = [p for x in poly for p in x]\n",
    "\n",
    "                x_min, y_min, x_max, y_max = (\n",
    "                    min(px), min(py), max(px), max(py))\n",
    "\n",
    "                data_anno = dict(\n",
    "                    image_id=idx,\n",
    "                    id=obj_count,\n",
    "                    category_id=0,\n",
    "                    bbox=[x_min, y_min, x_max - x_min, y_max - y_min],\n",
    "                    area=(x_max - x_min) * (y_max - y_min),\n",
    "                    segmentation=[poly],\n",
    "                    iscrowd=0)\n",
    "                annotations.append(data_anno)\n",
    "                obj_count += 1    \n",
    "        # else:    \n",
    "            # No annotations associated in this case\n",
    "            \n",
    "            # data_anno = dict(\n",
    "            #     image_id=idx,\n",
    "            #     id='',\n",
    "            #     category_id=0,\n",
    "            #     bbox=[],\n",
    "            #     area=0,\n",
    "            #     segmentation=[],\n",
    "            #     iscrowd=0)\n",
    "            # annotations.append(data_anno)\n",
    "            # obj_count += 1    \n",
    "\n",
    "    except: \n",
    "        print(train_csv.loc[idx,'json'])\n",
    "    \n",
    "out_file = DATA_DIR + 'train/train_annotation_coco.json'\n",
    "coco_format_json = dict(\n",
    "    images=images,\n",
    "    annotations=annotations,\n",
    "    categories=[{'id':0, 'name': 'shiptrack'}])\n",
    "\n",
    "mmcv.dump(coco_format_json, out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Val Annotations\n",
    "\n",
    "annotations = []\n",
    "images = []\n",
    "obj_count = 0\n",
    "for idx in range(len(val_csv.loc[:,'json'])):\n",
    "    try:\n",
    "        annotation = mmcv.load(val_csv.loc[idx,'json'])\n",
    "\n",
    "        filename = annotation['imagePath']\n",
    "        # print(filename)\n",
    "        # img_path = osp.join(image_prefix, filename)\n",
    "        # print(img_path)\n",
    "        height, width = int(annotation['imageHeight']), int(annotation['imageWidth'])\n",
    "        # print(height, width)\n",
    "\n",
    "        images.append(dict(\n",
    "        id=int(idx),\n",
    "        file_name=filename,\n",
    "        height=height,\n",
    "        width=width))\n",
    "\n",
    "        bboxes = []\n",
    "        labels = []\n",
    "        masks = []\n",
    "        if len(annotation['shapes']) > 0:\n",
    "            for obj in annotation['shapes']:\n",
    "                px = [points[0] for points in obj['points']]\n",
    "                py = [points[1] for points in obj['points']]\n",
    "                poly = [(x + 0.5, y + 0.5) for x, y in zip(px, py)]\n",
    "                poly = [p for x in poly for p in x]\n",
    "\n",
    "                x_min, y_min, x_max, y_max = (\n",
    "                    min(px), min(py), max(px), max(py))\n",
    "\n",
    "                data_anno = dict(\n",
    "                    image_id=idx,\n",
    "                    id=obj_count,\n",
    "                    category_id=0,\n",
    "                    bbox=[x_min, y_min, x_max - x_min, y_max - y_min],\n",
    "                    area=(x_max - x_min) * (y_max - y_min),\n",
    "                    segmentation=[poly],\n",
    "                    iscrowd=0)\n",
    "                annotations.append(data_anno)\n",
    "                obj_count += 1    \n",
    "        # else:    \n",
    "            # No annotations associated in this case\n",
    "            \n",
    "            # data_anno = dict(\n",
    "            #     image_id=idx,\n",
    "            #     id='',\n",
    "            #     category_id=0,\n",
    "            #     bbox=[],\n",
    "            #     area=0,\n",
    "            #     segmentation=[],\n",
    "            #     iscrowd=0)\n",
    "            # annotations.append(data_anno)\n",
    "            # obj_count += 1    \n",
    "\n",
    "    except: \n",
    "        print(val_csv.loc[idx,'json'])\n",
    "    \n",
    "out_file = DATA_DIR + 'val/val_annotation_coco.json'\n",
    "coco_format_json = dict(\n",
    "    images=images,\n",
    "    annotations=annotations,\n",
    "    categories=[{'id':0, 'name': 'shiptrack'}])\n",
    "\n",
    "mmcv.dump(coco_format_json, out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Annotations\n",
    "\n",
    "annotations = []\n",
    "images = []\n",
    "obj_count = 0\n",
    "for idx in range(len(test_csv.loc[:,'json'])):\n",
    "    try:\n",
    "        annotation = mmcv.load(test_csv.loc[idx,'json'])\n",
    "\n",
    "        filename = annotation['imagePath']\n",
    "        # print(filename)\n",
    "        # img_path = osp.join(image_prefix, filename)\n",
    "        # print(img_path)\n",
    "        height, width = int(annotation['imageHeight']), int(annotation['imageWidth'])\n",
    "        # print(height, width)\n",
    "\n",
    "        images.append(dict(\n",
    "        id=int(idx),\n",
    "        file_name=filename,\n",
    "        height=height,\n",
    "        width=width))\n",
    "\n",
    "        bboxes = []\n",
    "        labels = []\n",
    "        masks = []\n",
    "        if len(annotation['shapes']) > 0:\n",
    "            for obj in annotation['shapes']:\n",
    "                px = [points[0] for points in obj['points']]\n",
    "                py = [points[1] for points in obj['points']]\n",
    "                poly = [(x + 0.5, y + 0.5) for x, y in zip(px, py)]\n",
    "                poly = [p for x in poly for p in x]\n",
    "\n",
    "                x_min, y_min, x_max, y_max = (\n",
    "                    min(px), min(py), max(px), max(py))\n",
    "\n",
    "                data_anno = dict(\n",
    "                    image_id=idx,\n",
    "                    id=obj_count,\n",
    "                    category_id=0,\n",
    "                    bbox=[x_min, y_min, x_max - x_min, y_max - y_min],\n",
    "                    area=(x_max - x_min) * (y_max - y_min),\n",
    "                    segmentation=[poly],\n",
    "                    iscrowd=0)\n",
    "                annotations.append(data_anno)\n",
    "                obj_count += 1    \n",
    "        # else:    \n",
    "            # No annotations associated in this case\n",
    "            \n",
    "            # data_anno = dict(\n",
    "            #     image_id=idx,\n",
    "            #     id='',\n",
    "            #     category_id=0,\n",
    "            #     bbox=[],\n",
    "            #     area=0,\n",
    "            #     segmentation=[],\n",
    "            #     iscrowd=0)\n",
    "            # annotations.append(data_anno)\n",
    "            # obj_count += 1    \n",
    "\n",
    "    except: \n",
    "        print(test_csv.loc[idx,'json'])\n",
    "    \n",
    "out_file = DATA_DIR + 'test/test_annotation_coco.json'\n",
    "coco_format_json = dict(\n",
    "    images=images,\n",
    "    annotations=annotations,\n",
    "    categories=[{'id':0, 'name': 'shiptrack'}])\n",
    "\n",
    "mmcv.dump(coco_format_json, out_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1 Checking discrepancy between empty images and empty jsons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_annotations = mmcv.load('/deep/group/aicc-bootcamp/cloud-pollution/data/combined_v3_typed_new_composite/COCO_corrected_all_w_null/train/train_annotation_coco.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2499"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_annotations['images'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1433\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "\n",
    "for idx in range(len(train_csv)):\n",
    "    if train_csv.loc[idx, 'contains_shiptrack'] == True:\n",
    "        i+=1\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1427"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_ids = []\n",
    "for i in range(len(train_annotations['annotations'])):\n",
    "    image_ids.append(train_annotations['annotations'][i]['image_id'])\n",
    "\n",
    "len(set(image_ids))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1add5b6655bdb4994e45ffcc4f442336fedd3dac9962324812d32e304a74a3b6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
